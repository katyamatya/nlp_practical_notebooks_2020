{"cells":[{"cell_type":"markdown","source":"Text (here we will talk about the imporance of preprocessing briefly)","metadata":{"id":"smXl0DVmMgyw","colab_type":"text","cell_id":"00000-302f6439-a6f6-4fe2-a1bb-24045b14417b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Basics of text pre-processing","metadata":{"id":"gJWlqcvCOGs2","colab_type":"text","cell_id":"00001-6c2e74e8-b406-49c3-86f1-02b706bb0056","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Word tokenization\n\n","metadata":{"id":"1wB_7eU8O-i-","colab_type":"text","cell_id":"00002-e9b093cf-1412-446d-b9da-317feaa33c65","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Tokenisation** is separating out (tokenising) words (tokens) from running text. ","metadata":{"id":"5904u3rLTisj","colab_type":"text","cell_id":"00003-63c5a6a5-b9f9-459b-bcfa-73918ab27e60","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"u0e6LVQLV5JV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":434},"outputId":"53c7c0c3-f96f-4f50-d24f-1bcc23cf0d18","cell_id":"00004-4f87cda3-d143-4b86-a66e-fc84c9fd753e","deepnote_cell_type":"code"},"source":"#put necassary library imports here\n!pip install nltk\n!pip install spacy \n#pip install spacy==2.0.11  if you need a specific version\n","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/venv/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk) (0.16.0)\nRequirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (from nltk) (4.48.2)\nRequirement already satisfied: regex in /opt/venv/lib/python3.7/site-packages (from nltk) (2020.7.14)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\nRequirement already satisfied: spacy in /opt/venv/lib/python3.7/site-packages (2.3.2)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (4.48.2)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy) (3.0.2)\nRequirement already satisfied: blis<0.5.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (0.4.1)\nRequirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.1.3)\nRequirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (0.8.0)\nRequirement already satisfied: numpy>=1.15.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.19.0)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.0.2)\nRequirement already satisfied: thinc==7.4.1 in /opt/venv/lib/python3.7/site-packages (from spacy) (7.4.1)\nRequirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.0.2)\nRequirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/venv/lib/python3.7/site-packages (from spacy) (1.0.0)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/venv/lib/python3.7/site-packages (from spacy) (2.24.0)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/venv/lib/python3.7/site-packages (from spacy) (2.0.3)\nRequirement already satisfied: setuptools in /opt/venv/lib/python3.7/site-packages (from spacy) (47.3.1)\nRequirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/venv/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\nRequirement already satisfied: zipp>=0.5 in /opt/venv/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"id":"XOkOTC_NX2Tk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"5aeb81d0-e81d-4022-8038-f780dadcb180","cell_id":"00005-ea7364c5-3331-4f5b-baf8-8dd63bcee21a","deepnote_cell_type":"code"},"source":"import nltk\nimport spacy\nimport re\n\n#Loading requirements\n\nnltk.download('punkt')\n","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"2oIDHaD8O8F8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"0fbac606-ebb0-4027-8899-14d303960e1b","cell_id":"00006-1c6cc509-7a06-4f9c-bf17-08e6d4f3fb13","deepnote_cell_type":"code"},"source":"#Simple Search\ntext = '''The world's hardest hit country, the U.S., had recorded 5.99 million cases of infection as of 0030 GMT Monday, according to Johns Hopkins University's tracker.And the death toll is just over 183,000.\nOn the other side of the world, Latin America -- the worst-hit region -- is still struggling with its first wave, with Covid-19 deaths in Brazil crossing 120,000, second only to the United States.'''\n#Look for white space   -> what can go wrong?\ntokens_while_space_splitted = text.split(\" \")\nprint(\"White space splitted tokens:\",tokens_while_space_splitted)\n#fix this search method to use punctuation as seperator as well as white space , What new problem does it cause?\n\n\n#Regex method\nregex = re.compile('\\w+')\nprint(\"Regex tokens=\",regex.findall(text))\n#fix this regex to accomadate U.S in one token\n\n#Nltk lib\nfrom nltk.tokenize import word_tokenize\nnltk_tokens = nltk.tokenize.word_tokenize(text)\nprint(\"NLTK tokens\", nltk_tokens)\n#Spacy\nspacy_nlp = spacy.load('en_core_web_sm')\ndoc = spacy_nlp(text)\nspacy_tokens = [token.text for token in doc]\nprint(\"Spacy tokens:\",spacy_tokens)\n\n#Regex Exersice: Extract the Named Entities in the above text using Regex. \n\n","execution_count":null,"outputs":[{"name":"stdout","text":"White space splitted tokens: ['The', \"world's\", 'hardest', 'hit', 'country,', 'the', 'U.S.,', 'had', 'recorded', '5.99', 'million', 'cases', 'of', 'infection', 'as', 'of', '0030', 'GMT', 'Monday,', 'according', 'to', 'Johns', 'Hopkins', \"University's\", 'tracker.And', 'the', 'death', 'toll', 'is', 'just', 'over', '183,000.\\nOn', 'the', 'other', 'side', 'of', 'the', 'world,', 'Latin', 'America', '--', 'the', 'worst-hit', 'region', '--', 'is', 'still', 'struggling', 'with', 'its', 'first', 'wave,', 'with', 'Covid-19', 'deaths', 'in', 'Brazil', 'crossing', '120,000,', 'second', 'only', 'to', 'the', 'United', 'States.']\nRegex tokens= ['The', 'world', 's', 'hardest', 'hit', 'country', 'the', 'U', 'S', 'had', 'recorded', '5', '99', 'million', 'cases', 'of', 'infection', 'as', 'of', '0030', 'GMT', 'Monday', 'according', 'to', 'Johns', 'Hopkins', 'University', 's', 'tracker', 'And', 'the', 'death', 'toll', 'is', 'just', 'over', '183', '000', 'On', 'the', 'other', 'side', 'of', 'the', 'world', 'Latin', 'America', 'the', 'worst', 'hit', 'region', 'is', 'still', 'struggling', 'with', 'its', 'first', 'wave', 'with', 'Covid', '19', 'deaths', 'in', 'Brazil', 'crossing', '120', '000', 'second', 'only', 'to', 'the', 'United', 'States']\nNLTK tokens ['The', 'world', \"'s\", 'hardest', 'hit', 'country', ',', 'the', 'U.S.', ',', 'had', 'recorded', '5.99', 'million', 'cases', 'of', 'infection', 'as', 'of', '0030', 'GMT', 'Monday', ',', 'according', 'to', 'Johns', 'Hopkins', 'University', \"'s\", 'tracker.And', 'the', 'death', 'toll', 'is', 'just', 'over', '183,000', '.', 'On', 'the', 'other', 'side', 'of', 'the', 'world', ',', 'Latin', 'America', '--', 'the', 'worst-hit', 'region', '--', 'is', 'still', 'struggling', 'with', 'its', 'first', 'wave', ',', 'with', 'Covid-19', 'deaths', 'in', 'Brazil', 'crossing', '120,000', ',', 'second', 'only', 'to', 'the', 'United', 'States', '.']\n","output_type":"stream"},{"output_type":"error","ename":"OSError","evalue":"[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-fa39356f694f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"NLTK tokens\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnltk_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#Spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mspacy_nlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mspacy_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."]}]},{"cell_type":"markdown","source":"**Sentence tokenization** is splitting text into individual sentences. \n","metadata":{"id":"BxJOZa_gPPFj","colab_type":"text","cell_id":"00007-a96ee5c4-f63e-40c0-bd6a-de2d28b3fed1","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"id":"HbMupeyLPtSS","colab_type":"code","colab":{},"cell_id":"00008-22da666a-0a8e-49dd-acba-e6beed15d264","deepnote_cell_type":"code"},"source":"#Simple Search\n#Regex method\n#Nltk lib\n#Spacy","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9B1rEX9mUz9p","colab_type":"code","colab":{},"cell_id":"00009-5658834a-6d08-43ca-9698-7bb5a0c7aa3f","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Case-folding (lowercasing)","metadata":{"id":"lkFfc1UkU4t9","colab_type":"text","cell_id":"00010-c01ac7d3-5424-4c26-a261-efd59f970947","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"##Stopword removal","metadata":{"id":"YP-QL9dUUsIj","colab_type":"text","cell_id":"00011-64ecc626-dfe4-4af1-85a9-d43405f3b835","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"##Stemming","metadata":{"id":"77i42Cp1Uye_","colab_type":"text","cell_id":"00012-5132c961-b9d0-482c-ac30-62544b443aac","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"id":"UWkI5lMaU1wY","colab_type":"text","cell_id":"00013-7e6ef79c-dd64-44af-92e1-3c9c829b2853","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Different approaches for different tasks","metadata":{"id":"vw9qnNfzOW3P","colab_type":"text","cell_id":"00014-dfb4761b-cc7e-476c-b826-11c26b1c0706","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"It has been argued that ","metadata":{"id":"5PXMZYwuVfFT","colab_type":"text","cell_id":"00015-8b6670ef-8599-45d3-af08-93248e033a36","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Homework","metadata":{"id":"xnHRh4e3Ob4B","colab_type":"text","cell_id":"00016-94ae978c-2be8-47a9-a346-42d77434431b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00017-8b23ee33-0200-4a19-82d9-de158dbc7ffe","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practical 1: text preprocessing.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"deepnote_notebook_id":"5c864efc-af3d-411d-a5c4-1b86810cb379","deepnote_execution_queue":[]}}