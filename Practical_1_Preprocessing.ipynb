{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "deepnote_notebook_id": "09fb9510-5433-4882-9ab0-7c288df283f7",
    "deepnote_execution_queue": [],
    "colab": {
      "name": "Practical_1_Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "00000-952b1f43-5c5b-4096-9e4c-446f475b5de5",
        "deepnote_cell_type": "markdown",
        "id": "TRTLQ46XJxBY"
      },
      "source": [
        "# Introduction to NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00001-94a87e4c-9d6b-4f1e-9aeb-4417b92b8acb",
        "deepnote_cell_type": "markdown",
        "id": "uHLUunSUJxBa"
      },
      "source": [
        "Let's start from importing the NLTK library.\n",
        "We also download its 'punkt' sentence tokenizer (about which we will talk later). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-94349405-9e81-40e9-8b75-e3a98c9cd67f",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "b9QGaHMDJxBc"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-0854d537-15a1-4b96-a8dc-9ad95652c534",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "iL0CYghsJxBd"
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00003-aaf50bbe-033d-4053-a6ff-5397ad74895b",
        "deepnote_cell_type": "markdown",
        "id": "KJLwSMb7JxBe"
      },
      "source": [
        "FILE variable will point to the location of the text file we will be analysing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00001-a90a92cf-a975-43b8-b16b-b4a81e5d446e",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "K6vlsymyJxBf"
      },
      "source": [
        "# configure; using an absolute path, define the location of a plain text file for analysis\n",
        "FILE = 'walden.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-933a0666-80ed-4227-9c39-8f32f2493ef7",
        "deepnote_cell_type": "markdown",
        "id": "KKyrrn6yJxBg"
      },
      "source": [
        "Now, let's import all modules from NLTK. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00002-4eff6b11-d08d-47a8-8e2c-18d4d5c91f16",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "GG0owpeHJxBg"
      },
      "source": [
        "# import / require the use of the Toolkit\n",
        "from nltk import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00007-e0dd683c-732f-43d6-9533-36d165fca86a",
        "deepnote_cell_type": "markdown",
        "id": "MIiS-g-6JxBh"
      },
      "source": [
        "To open a file, we use the built-in open function. \n",
        "The open function returns a file object that contains methods and attributes to perform various operations on the file.\n",
        "\n",
        "To read a file’s contents, call .read() method which reads some quantity of data and returns it as a string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00003-620aae11-8583-44d5-9963-c179493dbbc8",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "Seko9fQ1JxBi"
      },
      "source": [
        "# slurp up the given file; display the result\n",
        "handle = open(FILE, 'r')\n",
        "data = handle.read()\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00009-efadede2-e794-4dcf-8378-5035900876ec",
        "deepnote_cell_type": "markdown",
        "id": "cpjjJvNLJxBj"
      },
      "source": [
        "**Tokenizing** is splitting a text into chunks which can be of various granularity (e.g. sentences, tokens (words, numbers, pubncuation marks) or even letters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00004-840300d5-6dd1-4b42-9514-7a99f731aada",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "VNtnC6C4JxBj"
      },
      "source": [
        "features = word_tokenize(data)\n",
        "print(features)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00011-be4fb194-5a6b-4e56-b28c-16af42f8c6ec",
        "deepnote_cell_type": "markdown",
        "id": "8ON6_8HgJxBk"
      },
      "source": [
        "**CODE IT** Can you think of any alternative ways for tokenization? \n",
        "\n",
        "Insert your code for an alternative in the cell below marked by #Insert your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00011-2d62e47d-b7a5-4262-97a8-ba8f837f47d7",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "kWRKaDfOJxBk"
      },
      "source": [
        "\n",
        "def word_tokenize_function(text):\n",
        "    tokens = []\n",
        "    # Insert your code here\n",
        "    return (tokens)\n",
        "\n",
        "features_alternative = word_tokenize_function(data)\n",
        "print(features_alternative)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00011-e2f2442c-5fa7-40ae-ab9b-2057b903a440",
        "deepnote_cell_type": "markdown",
        "id": "pjRiIvZBJxBk"
      },
      "source": [
        "Depending on the task, we may need to **lowercase** all words. Think about how this may impact the analysis and what use cases may require lowercasing and which - not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00005-d7d743d2-6658-42c3-b2c2-07f686691080",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "z2nThWd_JxBk"
      },
      "source": [
        "# normalize the features to lower case and exclude punctuation\n",
        "features = [feature for feature in features if feature.isalpha()]\n",
        "features = [feature.lower() for feature in features]\n",
        "print( features )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00013-c9bcd434-25a6-4741-987b-7fb5117302f4",
        "deepnote_cell_type": "markdown",
        "id": "kAzuciD8JxBl"
      },
      "source": [
        "Removing **stopwords** - words that are very common (e.g. 'i', 'and', 'by', 'for', 'haven't' etc.) and of relatively low value for text analysis. They are often removed, and there are various stategies (from ready-made stopword lists like NLTK's one to customised lists based on a particular corpus). There are also use cases when stopwords are not removed - can you think of cases when it may be not desirable?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00006-24d55fe0-1992-4503-95d9-0e38978764c3",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "5pnnBVQ_JxBl"
      },
      "source": [
        "# create a list of (English) stopwords, and then remove them from the features\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords = stopwords.words('english')\n",
        "features  = [feature for feature in features if feature not in stopwords]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00017-5f95b1d1-2f78-4e82-ad23-1666520f0f02",
        "deepnote_cell_type": "markdown",
        "id": "bHBSJleyJxBm"
      },
      "source": [
        "**CODE IT**  Create a list of words which would NOT EXCLUDE stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-8ccf04e1-7069-4e44-b8ef-c254e89d3f7a",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "LUMCoMcfJxBm"
      },
      "source": [
        "all_words=[]\n",
        "\n",
        "#Insert your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-19e08d10-9eda-4b58-9b32-5fd5fc13f2a7",
        "deepnote_cell_type": "markdown",
        "id": "FzdLqfOZJxBn"
      },
      "source": [
        "## Frequency distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00017-87404256-8922-4531-bda9-a9ffe77c65bc",
        "deepnote_cell_type": "markdown",
        "id": "BcwlarUgJxBn"
      },
      "source": [
        "Sometimes we may want to find how many times each word occurs in the text (that is, its **frequency**).\n",
        "What insights can this information give us?\n",
        "It's called distribution because we learn how the total number of tokens in the text is distributed across all vocabulary items. \n",
        "**Vocabulary** is a list of unique words your text contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "cell_id": "00007-9dd46198-3dca-4dcd-87b1-4c613dc29010",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "pg_U3bQkJxBo"
      },
      "source": [
        "# count & tabulate the features, and then plot the results -- season to taste\n",
        "frequencies = FreqDist(features)\n",
        "plot = frequencies.plot(10)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00024-7d839afa-e8bb-46a0-9b48-6e8341758846",
        "deepnote_cell_type": "markdown",
        "id": "ITUzGEjaJxBo"
      },
      "source": [
        "**CODE IT** Create a frequency distribution plot for all words in the text including stopwords using the list you created in the previous exercise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00026-d39e0969-a446-4a19-9886-ede694d853ba",
        "deepnote_cell_type": "markdown",
        "id": "yG13LdIsJxBo"
      },
      "source": [
        "**OBSERVE AND REFLECT** What do you observe comparing the two plots?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00027-27cf95e3-f515-43e1-b2df-748bec1e3951",
        "deepnote_cell_type": "markdown",
        "id": "nwZWt7AAJxBp"
      },
      "source": [
        "_Write your reflection here:_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-0b3a0a1f-8ffd-4135-9ee0-360a73e26c72",
        "deepnote_cell_type": "markdown",
        "id": "I7U_-x8KJxBp"
      },
      "source": [
        "Words  in  English  texts  have  a  very  peculiar  distribution. \n",
        "\n",
        "**Recommended video** (0:00-3:15 of this video is about **Zipf's Law** in language https://www.youtube.com/watch?v=fCn8zs912OE - but the rest of the video is also worth checking out if you have time!)\n",
        "\n",
        "Check frequency of any word in the British National Corpus (100mln words)  designed to represent an accurate cross-section of current English usage here: http://www.wordcount.org/main.php\n",
        "\n",
        "Around 50–100 top frequency words typically account for about 50% of the words in any text. \n",
        "But at the same time, about half of the words of the vocabulary of a text occur only once in the text! Such a word is called **\"hapax legomenon\"** (or just hapax). \n",
        "Why do we sometimes want to find them? What can we do with them in the context of NLP? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00008-59cbdb18-adc8-4cbd-88c9-372c0eb032f9",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "hg1rP_lGJxBp"
      },
      "source": [
        "# create a list of unique words (hapaxes); display them\n",
        "hapaxes = frequencies.hapaxes()\n",
        "print(hapaxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00029-46ad16a0-b1d3-43a0-b3e7-38d64c554a0c",
        "deepnote_cell_type": "markdown",
        "id": "YFCMXM5MJxBp"
      },
      "source": [
        "**CODE IT** Write a function that extracts unique words from the data, without using NLTK built-in functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00030-80a78531-f5f0-440a-ae85-ab76867d3752",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "aMOoR4nlJxBq"
      },
      "source": [
        "def unique_words(data):\n",
        "#insert your code here\n",
        "return #add return value here\n",
        "\n",
        "print(unique_words(data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00009-d8da8f32-547f-44a1-95eb-de7fa286879c",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "Vbhv_aUQJxBq"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "# count & tabulate ngrams from the features -- season to taste; display some\n",
        "ngrams = ngrams(features, 2)\n",
        "print(ngrams)\n",
        "frequencies = FreqDist(ngrams)\n",
        "frequencies.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00032-5e353c35-673b-4346-954b-1ac577dca4ab",
        "deepnote_cell_type": "markdown",
        "id": "ZYWchTQQJxBq"
      },
      "source": [
        "**CODE IT** Print Trigrams, Fourgrams ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00033-6d47bf13-afcb-4954-ba0c-ab799ad61e8a",
        "deepnote_cell_type": "code",
        "id": "lEbjSDeKJxBq"
      },
      "source": [
        "#insert your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00034-30dceb44-ddf4-4822-92d2-483a95d908ac",
        "deepnote_cell_type": "markdown",
        "id": "DcA_IsfYJxBq"
      },
      "source": [
        "What are the most frequent trigrams, fourgrams ,... \n",
        "Reflect on the size of the ngram and the frequency\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00035-bfdfbc33-1c41-4ba4-aca5-88566caaf0b2",
        "deepnote_cell_type": "markdown",
        "id": "nBA0Xi_kJxBr"
      },
      "source": [
        "**CODE IT** Find Bigrams, Trigrams and Fourgrams... using all words i.e. including stop words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00036-224ea803-3ec0-4353-af94-da4d2e3b6d85",
        "deepnote_cell_type": "markdown",
        "id": "qNbaFU_fJxBr"
      },
      "source": [
        "**OBSERVE AND REFLECT:** What difference will you observe make if we use all words including stopwords rearding the frequency of ngrams?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00037-a3be3545-2649-446d-8976-0796f0d869c4",
        "deepnote_cell_type": "markdown",
        "id": "0KTIzYdvJxBr"
      },
      "source": [
        "_Write your reflection here:_\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00010-49b411da-df4a-4e78-a44b-90a93092ce72",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "6Qmc6BD0JxBr"
      },
      "source": [
        "# create a list each token's length, and plot the result; How many \"long\" words are there?\n",
        "lengths = [len(feature) for feature in features]\n",
        "plot = FreqDist(lengths).plot(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00023-1db4aced-634f-46fb-975d-95418bc4e9bb",
        "deepnote_cell_type": "markdown",
        "id": "UXVNaqH3JxBr"
      },
      "source": [
        "## Stemming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00024-3e0241f9-94b9-430c-b485-bc1092011d57",
        "deepnote_cell_type": "markdown",
        "id": "HSquLvJrJxBs"
      },
      "source": [
        "**Stemmin** is basically removing the suffix from a word and reduce it to its root word. The purpose of stemming is to bring variant forms of a word together. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00025-e8b4e1cb-648b-46ed-a915-c37d1bfbec04",
        "deepnote_cell_type": "markdown",
        "id": "zKeXBrRvJxBs"
      },
      "source": [
        "In this tutorial we are using the **Porter stemmer** - it is the most widely used algorith but others also exist."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00011-ae6480a3-36d0-4ec6-b15d-6f70afcd908a",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "-_FWpCBDJxBs"
      },
      "source": [
        "# initialize a stemmer, stem the features, count & tabulate, and output\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem( feature ) for feature in features]\n",
        "frequencies = FreqDist(stems)\n",
        "frequencies.most_common(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00043-9ba34d16-7a68-4dd7-8229-2c6ab502972e",
        "deepnote_cell_type": "markdown",
        "id": "ZQcSOiswJxBs"
      },
      "source": [
        "**CODE IT** Can you implement any of the stemming rules in English using only python code and not the NLTK library? (for example: plural rules)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00021-599222ee-dc70-407d-8da4-e618b91e2b27",
        "deepnote_cell_type": "code",
        "id": "4_fd3yMMJxBt"
      },
      "source": [
        "def chosen_stemming_rule(word):\n",
        "    #Insert your code here\n",
        "    return stemmed_word\n",
        "\n",
        "new_stems = [chosen_stemming_rule( feature ) for feature in features]\n",
        "print(new_stems)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00012-73aa90cd-a0d5-498b-a0b2-7e1bf4b459ef",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "D-g-ughGJxBt"
      },
      "source": [
        "# re-create the features and create a NLTK Text object, so other cool things can be done\n",
        "features = word_tokenize(data)\n",
        "text = Text(features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00013-220a3cff-72ec-4286-b2fa-3378b7a073f6",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "00Yd9aC2JxBt"
      },
      "source": [
        "# count & tabulate, again; list a given word -- season to taste\n",
        "frequencies = FreqDist( text )\n",
        "print( frequencies[ 'love' ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00030-2499467f-24dc-4918-b0f6-4414e32e0d67",
        "deepnote_cell_type": "markdown",
        "id": "RHbUGHO4JxBu"
      },
      "source": [
        "## Concordances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00031-4af4632d-e7b6-447a-927b-e361106dc7be",
        "deepnote_cell_type": "markdown",
        "id": "m2UYyfkmJxBu"
      },
      "source": [
        "To examine the context of words in a text we can explore **concordances**.\n",
        "A concordance view in NLTK shows us every occurrence of a given word, together with some context (words surrounding it).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00014-5e698ce5-ca33-4a25-9ff4-e90828eb1e86",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "LDp94xLJJxBu"
      },
      "source": [
        "# do keyword-in-context searching against the text (concordancing)\n",
        "print( text.concordance( 'love' ) )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00050-568b5ad9-3b29-47aa-ba50-964634102616",
        "deepnote_cell_type": "markdown",
        "id": "to9CGq3hJxBu"
      },
      "source": [
        "**If you Fancy**\n",
        "Take a look at antConc and check if you can use it to find concordances in text: \n",
        "\n",
        "**ANTCONC:** https://www.laurenceanthony.net/software/antconc/\n",
        "\n",
        "\n",
        "**BLOG POST on ANTCONC** https://dhh.uni.lu/2020/08/11/antconc-historians-and-their-diverging-research-methods/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00033-94aaf38f-6103-41c6-98d4-36a196b8d02e",
        "deepnote_cell_type": "markdown",
        "id": "ulxyAOAqJxBv"
      },
      "source": [
        "## Dispersion plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00034-686a0cad-c6fc-4a53-aeda-8fd3e3844179",
        "deepnote_cell_type": "markdown",
        "id": "w-ImHdcxJxBv"
      },
      "source": [
        "**Lexical dispersion** measures how frequently a word appears across the parts of a corpus. \n",
        "This plot notes the occurrences of a word and how many words from the beginning of the corpus it appears (word offsets). \n",
        "when can it be useful?\n",
        "Think of a corpus that covers a long time period  - a dispersion plot can be used to analyse how frequency of specific terms may have changed over time. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00015-b87e8346-1694-4cd7-8321-9f65d194edcb",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "CUU-GWdeJxBv"
      },
      "source": [
        "# create a dispersion plot of given words\n",
        "plot = text.dispersion_plot(['love', 'war', 'man', 'god'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00036-34096aed-eeeb-4588-bcc7-4128281a7cb9",
        "deepnote_cell_type": "markdown",
        "id": "PSiFqHEdJxBv"
      },
      "source": [
        "**Bigrams** are sequences of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00016-be995831-e2e1-48e9-b406-a74f752b6e0d",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "rp8On5MMJxBw"
      },
      "source": [
        "# output the \"most significant\" bigrams, considering surrounding words (size of window) -- season to taste\n",
        "text.collocations(num=10, window_size=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00017-2f5a58e4-7a04-4a66-b558-8a77a181eaac",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "3hDKDhgLJxBw"
      },
      "source": [
        "# Find contexts where the specified words appear\n",
        "text.common_contexts(['love', 'war', 'man', 'god'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00058-11a65736-66df-42e3-a723-b21f4cd645ab",
        "deepnote_cell_type": "markdown",
        "id": "gSL_fsjnJxBw"
      },
      "source": [
        "**OBSERVE AND REFLECT**\n",
        "\n",
        "What's the difference between NGRAMS(bigrams, trigrams) and Collocations?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00059-bf9d2712-881c-41b3-96be-2b789fe0a38e",
        "deepnote_cell_type": "markdown",
        "id": "HDeNWQ_xJxBw"
      },
      "source": [
        "_Write your reflection here_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "cell_id": "00018-baf4f1d2-e5bc-4a24-9608-2aeb4a85e962",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "_XvTbo82JxBw"
      },
      "source": [
        "# list the words (features) most associated with the given word\n",
        "text.similar( 'love' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00019-ede8ece3-d43e-4bad-afee-937507d3b8b1",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "IS2Imi2jJxBx"
      },
      "source": [
        "# create a list of sentences, and display one -- season to taste\n",
        "sentences = sent_tokenize(data)\n",
        "sentence  = sentences[ 14 ]\n",
        "print(sentence)\n",
        "\n",
        "\n",
        "#Can you think of an alternate method using python only to implement sent_tokenize?\n",
        "\n",
        "def sent_tokenize_code(text):\n",
        "    sentences=[]\n",
        "    # Insert your code here\n",
        "    return sentences \n",
        "\n",
        "sentences = sent_tokenize_code(data)\n",
        "print(sentence[14])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00062-b9918cab-5e50-4aae-bcf7-dfce815cd7a6",
        "deepnote_cell_type": "markdown",
        "id": "5LakZhRvJxBx"
      },
      "source": [
        "**CODE IT** Can you think of an alternate method using python only to implement sent_tokenize without using NLTK built-in function?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00062-a676f26c-edc4-4bc1-b550-617fe363353b",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "6y6AWvjHJxBx"
      },
      "source": [
        "def sent_tokenize_code(text):\n",
        "    sentences=[]\n",
        "    # Insert your code here\n",
        "    return sentences \n",
        "\n",
        "alternative_sentences = sent_tokenize_code(data)\n",
        "print(alternative_sentences[14])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00030-355e70aa-1da1-485a-a7a7-1983f8ad143e",
        "deepnote_cell_type": "markdown",
        "id": "07Tv5l3MJxBy"
      },
      "source": [
        "## Part-of-speech tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00030-8d30bff7-5c5f-4eab-b0e3-d465aad971e0",
        "deepnote_cell_type": "markdown",
        "id": "f-8wT-WdJxBy"
      },
      "source": [
        "Do you remember from school what lexical categories or parts of speech are?\n",
        "You have probably about nouns, verbs, adjectives, pronouns and so on, and what function they have. \n",
        "\n",
        "- How are they used in natural language processing?\n",
        "- What is a good Python data structure for storing words and their categories?\n",
        "- How can we automatically tag each word of a text with its word class?\n",
        "\n",
        "The process of classifying words into their parts of speech and labeling them accordingly is known as **part-of-speech tagging, POS-tagging, or simply tagging**. \n",
        "**Parts of speech** are also known as **word classes** or **lexical categories**. \n",
        "**Tagset** - a collection of tags used for a particular task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00031-08bf2e41-1e26-4dc4-a63b-8a421a7fea0d",
        "deepnote_cell_type": "markdown",
        "id": "nUn7SDiDJxBy"
      },
      "source": [
        "**A part-of-speech tagger, or POS-tagger**, processes a sequence of words and attaches a part of speech tag to each word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00044-63c737c3-5689-4274-a36d-dbfc713d81e0",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "KKRjxNNLJxBy"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "00020-1d3613d8-558a-4731-9a7d-a5ccba8bf38a",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "E6hla3BDJxBz"
      },
      "source": [
        "# tokenize the sentence and parse it into parts-of-speech, all in one go\n",
        "sentence = pos_tag( word_tokenize( sentence ) )\n",
        "print( sentence )\n",
        "\n",
        "\n",
        "#Find Verbs in the data and print out the stems of the found verbs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00033-0e9328c6-ea01-4182-b323-d37b05121f48",
        "deepnote_cell_type": "markdown",
        "id": "JLnHh8dSJxBz"
      },
      "source": [
        "What do the tags mean?\n",
        "You can query NLTK by using nltk.help.upenn_tagset('RB'), or a regular expression, e.g. nltk.help.upenn_tagset('NN.*'). \n",
        "Let's do a little exercise: ask NLTK for help with some tags from the output above\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00034-ac5d78c7-595c-4ac3-b82a-3cf4f1599781",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "qMBwOgX7JxBz"
      },
      "source": [
        "nltk.download('tagsets')\n",
        "#your code goes here: ask NLTK for help with some tags from the output above"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00047-0b74a171-823b-4a81-9b9e-0d0177e95c40",
        "deepnote_cell_type": "markdown",
        "id": "srawiOxkJxBz"
      },
      "source": [
        "## Named entity recognition - NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00049-0cad8198-70bb-486d-824e-050169f522b4",
        "deepnote_cell_type": "markdown",
        "id": "tF5sO4v4JxB0"
      },
      "source": [
        "**Named-entity recognition (NER)** seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n",
        "\n",
        "In information extraction, a **named entity** is a real-world object, such as persons, locations, organizations, products, etc., that can be denoted with a proper name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00049-f6e9e6d0-71cd-4447-879f-71e0cb2c5bd1",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "YaCW6CqpJxB0"
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "cell_id": "00021-4a67444d-1340-475a-8281-ecc1748c4a7f",
        "output_cleared": false,
        "deepnote_cell_type": "code",
        "id": "ZO0nda4wJxB0"
      },
      "source": [
        "# extract named enities from a sentence, and print the results\n",
        "rtl_text = open(\"RTL.txt\").read()\n",
        "\n",
        "sentences=sent_tokenize(rtl_text)\n",
        "\n",
        "sentence = sentences[1]\n",
        "sentence = pos_tag( word_tokenize( sentence ) )\n",
        "\n",
        "from nltk import ne_chunk\n",
        "entities = ne_chunk(sentence)\n",
        "print(entities)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00076-80261628-a5d1-40fa-bb12-c3395674c249",
        "deepnote_cell_type": "markdown",
        "id": "Uc6JkSQzJxB0"
      },
      "source": [
        "**EXERCISE**  \n",
        "\n",
        "From [this](https://www.gutenberg.org/files/28054/28054-0.txt\n",
        ") link download the text file contaning the content of \"The Brothers Karamazov\" by Fyodor Dostoyevsky provided by [Project Gutenberg](https://www.gutenberg.org/about/)\n",
        "\n",
        "\n",
        "\n",
        "Use [this](https://python-graph-gallery.com/260-basic-wordcloud/) code piece to draw a word cloud for the text of \"The Brothers Karamazov\":\n",
        "\n",
        "- Without any preprocessing on the text\n",
        "-  After performing any preprocessing you find helpful to present a more informative word-cloud\n",
        "- Compare the two word cloud and write a reflection on your observation\n",
        "\n",
        "\n"
      ]
    }
  ]
}