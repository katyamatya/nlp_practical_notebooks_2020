{"cells":[{"cell_type":"markdown","source":"# Word embeddings modelling","metadata":{"tags":[],"cell_id":"00000-22249393-0b65-4e24-b87d-6ddb638964e5","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import nltk","metadata":{"tags":[],"cell_id":"00001-a3906043-950b-4bb7-8203-828054593c18","deepnote_to_be_reexecuted":false,"source_hash":"5600ace6","execution_millis":1059,"execution_start":1610716244596,"deepnote_cell_type":"code"},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Language models","metadata":{"tags":[],"cell_id":"00001-60a47584-2cf9-46fc-a3b2-b1a44e89adae","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Language models assign **probability values** to sequences of words. In essence, they are trying to \n“fill in the blank” based on context. \n\nGiven a sentence “The hand-held gaming device is powered by small solar /---/ \", a language model may complete this sentence by saying that the word **\"panels\"** would fill the gap 80% of the time and the word **\"batteries\"** 20% of the time.\n","metadata":{"tags":[],"cell_id":"00002-3abbc42f-9063-40c8-a06d-8eeab62cd0e2","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\nTwo types:\n\n- **statistical** (N-grams, Hidden Markov Models, linguistc rules)\n- **neural** (neural networks)","metadata":{"tags":[],"cell_id":"00003-6a1cec31-fd71-4277-a323-8249809c7faf","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Word vectors/embeddings","metadata":{"tags":[],"cell_id":"00004-18e03455-894a-463e-8696-a2804cb28521","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"When we want to process natural language and mine it for useful information using machine learning techniques we have to map textual data to some numerical representation (this process is called **vectorisation** as we create vectors of numeric values). Word vectors are often referred to as **\"word embeddings\"**. ","metadata":{"tags":[],"cell_id":"00005-0c57ca3a-83eb-46ba-a934-bcd1e7c724c3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Do you remember what a **vector** is?","metadata":{"tags":[],"cell_id":"00006-5b8d8f33-e5dc-47c2-9ac0-41bce5d68a16","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"- Geometry: **an object with magnitude and direction**\n\n- Computer science: **one-dimensional array** (e.g. $$[.44, .26, .07, -.89, -.15].$$ )","metadata":{"tags":[],"cell_id":"00007-a15797f3-5c30-4a19-9e32-a34519420a02","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"![Picture title](image-20201006-124756.png)\nImage source and refresher on vectors: https://www.mathsisfun.com/algebra/vectors-cross-product.html","metadata":{"tags":[],"cell_id":"00008-2bbf8e6c-b61e-4998-b76e-ca398e44877a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Data representation","metadata":{"tags":[],"cell_id":"00009-ecf93e3f-cfd0-4d43-8373-ae17b91400a0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"How do we vectorise words? There are quite a few ways to represent text data as numbers, depending on what information we want them to contain.","metadata":{"tags":[],"cell_id":"00010-4e558b86-2279-4cdf-af8e-b25fd976a63a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Types of data representation\n- one-hot (presence/absence)\n- frequency-based (occurence frequency)\n- distributed (read on!)","metadata":{"tags":[],"cell_id":"00011-388b8b9b-86a4-499b-a207-34b547654460","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### One-hot encoding","metadata":{"tags":[],"cell_id":"00012-879dbdef-63f3-4e3a-90b1-03cfa8729a09","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**One-hot encoding**  is the simplest method  (“1-of-N” encoding). The resulting embeddings (vectors) are composed of a single \"one\" and a number of \"0 (zeros)\".\nThis encoding method marks a particular vector **index** with a value of true (1) if the token occurs in a document and false (0) if it does not. \nIn other words, each element of a one-hot encoded vector reflects either the presence or absence of the token in the analysed text.\n\n\n","metadata":{"tags":[],"cell_id":"00013-e67d8479-d276-4880-a6ce-a50c3dd8b944","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00014-00f5b699-fa1a-41f6-9edd-aba4ce6ce3cd","deepnote_to_be_reexecuted":false,"source_hash":"d705a0a","execution_millis":44,"execution_start":1610716245656,"deepnote_cell_type":"code"},"source":"import numpy as np\nsentence1 = \"We value talking to a human being at the other end of a conversation\".lower().split()\nsentence2 = \"Trump is being given a steroid that is usually used for severe cases of covid-19\".lower().split()\n\nvocab = set(sentence1+sentence2)\nvocab = sorted(vocab)\nprint (\"vocabulary (two sentences combined): \", vocab)\n\n#encoding words in the sentece based on their index (position) in the vocabulary\ninteger_encoded = []\nfor i in sentence1:\n    print (np.array(vocab)==i)\n    v = np.where( np.array(vocab) == i)[0][0]\n    print ('v: ', v)\n    integer_encoded.append(v)\nprint (\"sentence 1 encoded: \",integer_encoded)\n\ninteger_encoded = []\nfor i in sentence2:\n    v = np.where( np.array(vocab) == i)[0][0]\n    integer_encoded.append(v)\nprint (\"sentence 2 encoded: \",integer_encoded)\n\ndef get_vec(len_vocab,word):\n    empty_vector = [0] * len_vocab\n    vect = 0\n    find = np.where(np.array(vocab) == word)[0][0]\n    empty_vector[find] = 1\n    return empty_vector\n\ndef get_matrix(vocab, sentence):\n    mat = []\n    len_vocab = len(vocab)\n    for i in sentence:\n        vec = get_vec(len_vocab,i)\n        mat.append(vec)\n        \n    return np.asarray(mat)\n\nprint (\"MATRIX Sentence 1 :\")\nprint (get_matrix(vocab, sentence1))   \nprint (\"MATRIX Sentence 2 :\")\nprint (get_matrix(vocab, sentence2))   ","execution_count":2,"outputs":[{"name":"stdout","text":"vocabulary (two sentences combined):  ['a', 'at', 'being', 'cases', 'conversation', 'covid-19', 'end', 'for', 'given', 'human', 'is', 'of', 'other', 'severe', 'steroid', 'talking', 'that', 'the', 'to', 'trump', 'used', 'usually', 'value', 'we']\n[False False False False False False False False False False False False\n False False False False False False False False False False False  True]\nv:  23\n[False False False False False False False False False False False False\n False False False False False False False False False False  True False]\nv:  22\n[False False False False False False False False False False False False\n False False False  True False False False False False False False False]\nv:  15\n[False False False False False False False False False False False False\n False False False False False False  True False False False False False]\nv:  18\n[ True False False False False False False False False False False False\n False False False False False False False False False False False False]\nv:  0\n[False False False False False False False False False  True False False\n False False False False False False False False False False False False]\nv:  9\n[False False  True False False False False False False False False False\n False False False False False False False False False False False False]\nv:  2\n[False  True False False False False False False False False False False\n False False False False False False False False False False False False]\nv:  1\n[False False False False False False False False False False False False\n False False False False False  True False False False False False False]\nv:  17\n[False False False False False False False False False False False False\n  True False False False False False False False False False False False]\nv:  12\n[False False False False False False  True False False False False False\n False False False False False False False False False False False False]\nv:  6\n[False False False False False False False False False False False  True\n False False False False False False False False False False False False]\nv:  11\n[ True False False False False False False False False False False False\n False False False False False False False False False False False False]\nv:  0\n[False False False False  True False False False False False False False\n False False False False False False False False False False False False]\nv:  4\nsentence 1 encoded:  [23, 22, 15, 18, 0, 9, 2, 1, 17, 12, 6, 11, 0, 4]\nsentence 2 encoded:  [19, 10, 2, 8, 0, 14, 16, 10, 21, 20, 7, 13, 3, 11, 5]\nMATRIX Sentence 1 :\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\nMATRIX Sentence 2 :\n[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**CODEIT** write a code snippet to extract the one-hot matrix representation of the following three sentences:\n\n1. NLP is now the most popular subfield of machine learning.\n2. My washing machine is not working properly now.\n3. Analysis of language using artificial intelligence methods have risen dramatically.\n\n","metadata":{"tags":[],"cell_id":"00015-c157e3f7-d781-446d-a21e-1bde3b91e5e5","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00016-73f54d86-5752-454b-9798-a6e3187e4034","deepnote_to_be_reexecuted":false,"source_hash":"457eaf7a","execution_millis":0,"execution_start":1610716245700,"deepnote_cell_type":"code"},"source":"##Insert your code here\nsentence_1 = \"NLP is now the most popular subfield of machine learning .\".lower().split()\nsentence_2 = \"My washing machine is not working properly now .\".lower().split()\nsentence_3 = \"Analysis of language using artificial intelligence methods have risen dramatically .\".lower().split()\n\nimport numpy as np\n\nvocab = set(sentence_1+sentence_2+sentence_3)\nvocab = sorted(vocab)\nprint (\"vocabulary (three sentences combined): \", vocab)\n#Insert your code here","execution_count":3,"outputs":[{"name":"stdout","text":"vocabulary (three sentences combined):  ['.', 'analysis', 'artificial', 'dramatically', 'have', 'intelligence', 'is', 'language', 'learning', 'machine', 'methods', 'most', 'my', 'nlp', 'not', 'now', 'of', 'popular', 'properly', 'risen', 'subfield', 'the', 'using', 'washing', 'working']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**OBSERVE AND REFLECT:**  Using the examples above explain why one-hot vector representation is **not** the best method for analysing semantic similarity? \n\n","metadata":{"tags":[],"cell_id":"00017-c07bbbac-543d-46ca-ae39-1ca1857be263","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Write your answer here ###","metadata":{"tags":[],"cell_id":"00018-9b358ad8-a4ef-43bf-872a-2c80048f2b21","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"---","metadata":{"tags":[],"cell_id":"00019-e6a6a448-3136-480f-bf34-8c906dec8fce","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"What are the **main problems with this one-hot representation**?\n","metadata":{"tags":[],"cell_id":"00020-9060eb9e-6b1a-4314-81ec-19067401dddb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"- **sparsity and size**: the representation size grows with the corpus (imagine a corpus with the 300,000 word vocabulary where each word vector will will have 300,000 dimensions (float values) with all but one being a zero) (computationally expensive!).\n- **each vector is equally distant from every other vector** (does not reflect their position in relation to each other)\n- **no contextual/semantic information** is embedded  - therefore they are not suitable for NLP tasks like POS tagging, named-entity recognition etc.\n\n\n","metadata":{"tags":[],"cell_id":"00021-ca5046d4-5bcc-4113-bf4a-53884551fc1e","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Distributed representation","metadata":{"tags":[],"cell_id":"00022-a717753b-b9a1-4519-bca2-2a92b2a7eb84","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"An alternative is called **distributed representation**. \n\nPlease read here UNTIL (and including) Figure 3 (up until \"While this shape example is oversimplified, it serves as a great high-level, abstract introduction to distributed representations\"\nto get familiar with this concept. https://www.oreilly.com/content/how-neural-networks-learn-distributed-representations/\n\n","metadata":{"tags":[],"cell_id":"00023-13dcd47a-9b2a-4cda-9ea5-b65f8c6ccb1d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Training a simple neural language model","metadata":{"tags":[],"cell_id":"00024-2896d0f8-70ae-493d-9be1-01caf00a1ae1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"1. represent words with **one-hot vectors**\n2. encode input words (create **word embeddings**):\n- take the  one-hot vector representing the input word\n- multiply it by a matrix of size (N,200) (200 is the vector size - number of dimensions - which is chosen **arbitrarily**).\nThis multiplication results in a vector of size 200 (word embedding). \n\n<img  src=\"http://mccormickml.com/assets/word2vec/matrix_mult_w_one_hot.png\"/>\n\n3. Now we have a representation of the input word. \nWe multiply it by a matrix of size (200,N) (**output embedding**).  \nAs a result, we get a vector of size N and then pass it through **softmax function**.\nSoftmax normalises values of the vector into a probability distribution (each one of the values is between 0 and 1, and their sum is 1). \nThis decoding step takes a word representation and returns a distribution which represents the model’s predictions of the next word. \n\n\n\n<center><img src = \"http://mccormickml.com/assets/word2vec/output_weights_function.png\"></center>\n\n\n\n","metadata":{"tags":[],"cell_id":"00025-b63779ed-2fcb-42b4-97a0-7965e44edfc0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**QUICK Softmax refresher**: https://victorzhou.com/blog/softmax/","metadata":{"tags":[],"cell_id":"00026-3f228a93-b1d3-4d5d-9d8e-d4c8c1bff8ad","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Data needed for training**: pairs of input and target output words\n\n**Data generation**: take every pair of neighboring words from the text and use the first one as the input word and the second one as the target output word. \nExample: “The cat is on the mat”.\n\n**Word pairs for training**: (The, cat), (cat, is), (is, on), (on, the), (the, mat).\n\n","metadata":{"tags":[],"cell_id":"00027-3ea67cb8-91ef-4a64-a209-3f2202a5eb3c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Training process**: using gradient descent to update the model during training and loss measures to calculate\nthe distance between the output distribution predicted by the model and the target distribution for each pair of training words. \nThe target distribution for each pair is a one-hot vector representing the target word.\n\n","metadata":{"tags":[],"cell_id":"00028-e86b339f-f0d6-4038-a0a7-7c8b935cbcb7","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Please check this page for more info on the algorithm architecture: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/","metadata":{"tags":[],"cell_id":"00029-bd7f5166-b74e-4ff3-b5de-09f391e19919","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Model performance evaluation**: Let's talk about PERPLEXITY again :) \n\n<img  src=\"https://miro.medium.com/max/616/1*vV0XMYe69LPMlH3fFouDtw.png\"/>","metadata":{"tags":[],"cell_id":"00030-487a5420-bd5a-4415-9afd-948c4ce062aa","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## How to improve the performance of a simple model?","metadata":{"tags":[],"cell_id":"00031-2cda670a-db48-4f1e-8cc5-192e9ab7681d","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Can you think what the biggest problem of this simple model is?","metadata":{"tags":[],"cell_id":"00032-6cd9926b-0f21-4b6f-92d9-660437d6eafb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"To predict the next word in the sentence, it only uses ONE preceding word. In real life, we consider much more context when reading and understanding a text. \nA model that could be taught to \"remember\" more than one preceding word would be more efficient!","metadata":{"tags":[],"cell_id":"00033-e3f18755-3215-48e5-ac77-24addda84368","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Example:** what words follow the word \"eat\"? \n\nWe can answer “cookies”, “nuts” or \"eucalyptus\", and the model could also reply that these words may have high probability of being the target ones. However, if we knew that the actual word sequence was “Koalas eat\" would it change our opinion about the most probable answer?\n\n\n![ChessUrl](https://media.giphy.com/media/eDUHhtooZxyhi/giphy.gif)","metadata":{"tags":[],"cell_id":"00034-639493ff-48b6-423c-b75e-c7bd4786dd0b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## WORD EMBEDDINGS\n\nWords get their embeddings by us looking at which other words they tend to appear next to. The mechanics of that is that\n\n1. We get a lot of text data (say, all Wikipedia articles, for example). then\n\n\n2. We have a window (say, of three words) that we slide against all of that text.\n\n\n3. The sliding window generates training samples for our model\n","metadata":{"tags":[],"cell_id":"00035-b8470a7c-b6aa-4cfa-983a-575cdc58deeb","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Word2vec\n\nA method of creating word embeddings","metadata":{"tags":[],"cell_id":"00036-525f151c-e079-417c-b674-fc14ae0ba2af","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"http://jalammar.github.io/illustrated-word2vec/\n","metadata":{"tags":[],"cell_id":"00037-0eb7f0ca-a1cb-4625-aff7-d6c5ef40ea5f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### GloVe (Global Vectors)","metadata":{"tags":[],"cell_id":"00038-aa13eac1-95cf-43e3-8c1d-396acfeae9d5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Disadvantage of skipgram models: they do not operate directly on the co-occurrence statistics.\nThey scan context windows across the entire corpus and fail to take advantage of the vast amount of repetition in the data.\n","metadata":{"tags":[],"cell_id":"00039-efa3aae0-3f20-41fd-9f8c-fea16a2457d1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**GloVe (Global Vectors)** is a **count-based model**. It learns word embeddings by dimensionality reduction of a **co-occurrence counts matrix**.\n\n1. Build a co-occurence matrix (each row = how often does a word occur with every other word in some defined context-size in a large corpus).\n\n2. Factorise this matrix (=> a lower-dimensional matrix: rows = word vectors).\n\n","metadata":{"tags":[],"cell_id":"00040-17560e4f-3148-4ff4-8d7c-b8145c1dac03","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Problems with word2vec and GloVe ","metadata":{"tags":[],"cell_id":"00041-24b2e3ff-20c6-43ff-b414-b16bb05b5f42","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"They create one vector for different meanings of a polysemous word (and about 40% of English words are polysemous!).\n\nExample: any occurence of the word \"bank\" (river bank or financial institution) - will be mapped to the same vector.\n\nWords exist in context and their meanings are defined by the contextual use. Would not it be beneficial to learn representations that reflect this?","metadata":{"tags":[],"cell_id":"00042-0c101a0c-db65-467c-8193-240f485c94e3","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### BERT (Bidirectional Encoder Representations from Transformers)","metadata":{"tags":[],"cell_id":"00043-49b6d1aa-b0eb-4234-a0ac-9d2f52f0237a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Release of BERT model was described as marking the beginning of a new era in NLP. **Bidirectional Encoder Representations from Transformers (BERT)** is a language model that looks both to the left and the right of a word to pre-train representations.\n\n![BertUrl](https://media.giphy.com/media/umMYB9u0rpJyE/giphy.gif)","metadata":{"tags":[],"cell_id":"00044-2741451a-88f7-4f20-b8ef-3414996ac079","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\nKey technical innovation:\n- applying bidirectional training of Transformer, a popular attention model, to language modelling\n- deeper understanding on a word's context\n- reads the entire sequence of words at once =>  learns context of a word based on all of its surroundings \n\n","metadata":{"tags":[],"cell_id":"00045-f90c3810-ed09-4ec5-8fc6-b02a79cd3787","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Fastext (by Facebook Research)","metadata":{"tags":[],"cell_id":"00046-aa350828-71c1-428d-9514-b9cfb426ce13","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"- represents each word as an n-gram of characters.\nExample: \"artificial\" with n=3 <ar, art, rti, tif, ifi, fic, ici, ial, al> (the angular brackets mean the beginning and end of the word).   \n- capture the meaning of shorter words and suffixes & prefixes\n- works well with rare words","metadata":{"tags":[],"cell_id":"00047-c2f1d502-6419-43ed-8b69-aa9b4bf4293b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Wait, there is more!","metadata":{"tags":[],"cell_id":"00048-b0eca7df-b3ab-4ef7-bd61-588c559417c6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"If you want to learn about the most recent models please check out the following links:","metadata":{"tags":[],"cell_id":"00049-038b8818-f3b1-4026-9c84-a82acde67f66","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"- Word2vec: http://jalammar.github.io/illustrated-word2vec/\n- Transformers: http://jalammar.github.io/illustrated-transformer/\n- GPT2: http://jalammar.github.io/illustrated-gpt2/  & https://openai.com/blog/gpt-2-1-5b-release/ (the model was initially not release to public out of fear it would be used to spread fake news, spam, and disinformation. )\n- GPT3 (2020): can generate computer code, prose and poetry; has been called \"amazing\", \"spooky\", \"humbling\", and \"more than a little terrifying\". \n- GPT3 use examples: https://gpt3examples.com/#examples\n","metadata":{"tags":[],"cell_id":"00050-0aae958d-e301-4bb2-8909-3e006c025289","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Let's see how word2vec models work","metadata":{"tags":[],"cell_id":"00051-893cca1e-7887-4f9c-8bd0-386323d5a62a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Choose a word embedding in a language of your preference and download it:http://vectors.nlpl.eu/repository/#\n","metadata":{"tags":[],"cell_id":"00052-6afb0a45-9c3e-45c7-be5e-6fe85c1cdf01","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Gensim** is a python library which implements various natural language processing methods and algorithms.","metadata":{"tags":[],"cell_id":"00053-5c43d1b8-7425-4f24-b499-cd10d79646b7","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00054-3470baf3-7890-47a8-9a3c-6ed186793764","deepnote_to_be_reexecuted":false,"source_hash":"9162bcaf","execution_millis":2512,"execution_start":1610716245701,"deepnote_cell_type":"code"},"source":"!pip install gensim\n","execution_count":4,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nRequirement already satisfied: gensim in /opt/venv/lib/python3.7/site-packages (3.8.3)\nRequirement already satisfied: smart-open>=1.8.1 in /opt/venv/lib/python3.7/site-packages (from gensim) (4.1.0)\nRequirement already satisfied: numpy>=1.11.3 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.19.4)\nRequirement already satisfied: six>=1.5.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from gensim) (1.15.0)\nRequirement already satisfied: scipy>=0.18.1 in /opt/venv/lib/python3.7/site-packages (from gensim) (1.5.4)\n\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In the following code we read the brown corpus from nltk library and train(build) a Word2Vec language model using Gensim library.\n\n**Note:** Running the following code takes a few minutes","metadata":{"tags":[],"cell_id":"00055-9a8f2fa4-385d-4a60-9e8e-53fdd7e16a50","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00056-52a969df-7f46-44b6-a1c3-524dd2cf7800","deepnote_to_be_reexecuted":false,"source_hash":"5f463216","execution_millis":46455,"execution_start":1610716372916,"deepnote_cell_type":"code"},"source":"import gensim\nimport logging\nfrom nltk.corpus import brown \n\n\nnltk.download('brown')\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\nsentences = brown.sents()\nmodel = gensim.models.Word2Vec(sentences, min_count=1)\n\nmodel.save('brown_model.bin')","execution_count":15,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n2021-01-15 13:12:52,896 : INFO : collecting all words and their counts\n2021-01-15 13:12:52,897 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n2021-01-15 13:12:53,640 : INFO : PROGRESS: at sentence #10000, processed 219770 words, keeping 23488 word types\n2021-01-15 13:12:54,306 : INFO : PROGRESS: at sentence #20000, processed 430477 words, keeping 34367 word types\n2021-01-15 13:12:55,003 : INFO : PROGRESS: at sentence #30000, processed 669056 words, keeping 42365 word types\n2021-01-15 13:12:55,669 : INFO : PROGRESS: at sentence #40000, processed 888291 words, keeping 49136 word types\n2021-01-15 13:12:56,197 : INFO : PROGRESS: at sentence #50000, processed 1039920 words, keeping 53024 word types\n2021-01-15 13:12:56,601 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 57340 sentences\n2021-01-15 13:12:56,602 : INFO : Loading a fresh vocabulary\n2021-01-15 13:12:56,871 : INFO : effective_min_count=1 retains 56057 unique words (100% of original 56057, drops 0)\n2021-01-15 13:12:56,871 : INFO : effective_min_count=1 leaves 1161192 word corpus (100% of original 1161192, drops 0)\n2021-01-15 13:12:57,134 : INFO : deleting the raw counts dictionary of 56057 items\n2021-01-15 13:12:57,136 : INFO : sample=0.001 downsamples 38 most-common words\n2021-01-15 13:12:57,137 : INFO : downsampling leaves estimated 854152 word corpus (73.6% of prior 1161192)\n2021-01-15 13:12:57,389 : INFO : estimated required memory for 56057 words and 100 dimensions: 72874100 bytes\n2021-01-15 13:12:57,393 : INFO : resetting layer weights\n2021-01-15 13:13:13,670 : INFO : training model with 3 workers on 56057 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n2021-01-15 13:13:14,679 : INFO : EPOCH 1 - PROGRESS: at 19.95% examples, 183859 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:15,689 : INFO : EPOCH 1 - PROGRESS: at 39.99% examples, 182671 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:16,705 : INFO : EPOCH 1 - PROGRESS: at 53.82% examples, 167237 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:17,746 : INFO : EPOCH 1 - PROGRESS: at 71.94% examples, 164093 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:18,753 : INFO : EPOCH 1 - PROGRESS: at 98.03% examples, 164857 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:18,823 : INFO : worker thread finished; awaiting finish of 2 more threads\n2021-01-15 13:13:18,829 : INFO : worker thread finished; awaiting finish of 1 more threads\n2021-01-15 13:13:18,831 : INFO : worker thread finished; awaiting finish of 0 more threads\n2021-01-15 13:13:18,832 : INFO : EPOCH - 1 : training on 1161192 raw words (853954 effective words) took 5.2s, 165556 effective words/s\n2021-01-15 13:13:19,846 : INFO : EPOCH 2 - PROGRESS: at 19.05% examples, 175402 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:20,854 : INFO : EPOCH 2 - PROGRESS: at 39.99% examples, 182458 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:21,878 : INFO : EPOCH 2 - PROGRESS: at 60.42% examples, 188464 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:22,934 : INFO : EPOCH 2 - PROGRESS: at 85.91% examples, 184646 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:23,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n2021-01-15 13:13:23,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n2021-01-15 13:13:23,508 : INFO : worker thread finished; awaiting finish of 0 more threads\n2021-01-15 13:13:23,509 : INFO : EPOCH - 2 : training on 1161192 raw words (854917 effective words) took 4.7s, 182834 effective words/s\n2021-01-15 13:13:24,545 : INFO : EPOCH 3 - PROGRESS: at 19.05% examples, 172335 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:25,556 : INFO : EPOCH 3 - PROGRESS: at 39.31% examples, 176834 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:26,578 : INFO : EPOCH 3 - PROGRESS: at 59.73% examples, 184728 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:27,622 : INFO : EPOCH 3 - PROGRESS: at 84.69% examples, 182390 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:28,513 : INFO : worker thread finished; awaiting finish of 2 more threads\n2021-01-15 13:13:28,524 : INFO : worker thread finished; awaiting finish of 1 more threads\n2021-01-15 13:13:28,529 : INFO : worker thread finished; awaiting finish of 0 more threads\n2021-01-15 13:13:28,530 : INFO : EPOCH - 3 : training on 1161192 raw words (853828 effective words) took 5.0s, 170268 effective words/s\n2021-01-15 13:13:29,555 : INFO : EPOCH 4 - PROGRESS: at 19.95% examples, 180960 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:30,574 : INFO : EPOCH 4 - PROGRESS: at 42.29% examples, 191108 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:31,603 : INFO : EPOCH 4 - PROGRESS: at 61.85% examples, 191390 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:32,630 : INFO : EPOCH 4 - PROGRESS: at 88.13% examples, 188213 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:33,120 : INFO : worker thread finished; awaiting finish of 2 more threads\n2021-01-15 13:13:33,123 : INFO : worker thread finished; awaiting finish of 1 more threads\n2021-01-15 13:13:33,127 : INFO : worker thread finished; awaiting finish of 0 more threads\n2021-01-15 13:13:33,128 : INFO : EPOCH - 4 : training on 1161192 raw words (853993 effective words) took 4.6s, 185838 effective words/s\n2021-01-15 13:13:34,144 : INFO : EPOCH 5 - PROGRESS: at 19.05% examples, 175062 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:35,166 : INFO : EPOCH 5 - PROGRESS: at 38.57% examples, 173867 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:36,173 : INFO : EPOCH 5 - PROGRESS: at 58.31% examples, 181179 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:37,182 : INFO : EPOCH 5 - PROGRESS: at 75.14% examples, 170427 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:38,220 : INFO : EPOCH 5 - PROGRESS: at 94.58% examples, 160240 words/s, in_qsize 0, out_qsize 0\n2021-01-15 13:13:38,474 : INFO : worker thread finished; awaiting finish of 2 more threads\n2021-01-15 13:13:38,488 : INFO : worker thread finished; awaiting finish of 1 more threads\n2021-01-15 13:13:38,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n2021-01-15 13:13:38,498 : INFO : EPOCH - 5 : training on 1161192 raw words (853999 effective words) took 5.4s, 159113 effective words/s\n2021-01-15 13:13:38,499 : INFO : training on a 5805960 raw words (4270691 effective words) took 24.8s, 172007 effective words/s\n2021-01-15 13:13:38,568 : INFO : saving Word2Vec object under brown_model.bin, separately None\n2021-01-15 13:13:38,568 : INFO : not storing attribute vectors_norm\n2021-01-15 13:13:38,569 : INFO : not storing attribute cum_table\n2021-01-15 13:13:39,366 : INFO : saved brown_model.bin\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Using the following code you can access vectors of words in your gensim model.","metadata":{"tags":[],"cell_id":"00057-b23bd006-9e42-47c3-966b-32013824bbcc","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00058-29b94963-660d-4e2e-b26f-ea583870af74","deepnote_to_be_reexecuted":false,"source_hash":"11d403da","execution_millis":7,"execution_start":1610716292752,"deepnote_cell_type":"code"},"source":"import numpy as np\nimport nltk\n\n# Access vectors for specific words with a keyed lookup:\nvector = model['year']\nprint(vector)\n# see the shape of the vector (300,)\nprint(vector.shape)\n# Processing sentences is not as simple as with Spacy:\nvectors = [model[x] for x in \"This is some text I am processing with text analysis library\".split(' ')]\n","execution_count":6,"outputs":[{"name":"stdout","text":"[-0.13499422 -0.04724487  0.8355935  -0.51010275  1.0083615  -0.3086034\n -0.26302773  0.39535046 -0.79079735 -0.43304572 -0.07567624  0.3719021\n -0.20299797  0.48016202 -0.14941478  0.22955637 -0.23768008  1.6194006\n -0.32277167 -0.4755897   0.05924155 -2.0401626  -0.07549319 -0.2911573\n -0.4468673   0.05049513  0.64557964 -0.6613545   0.02351663 -0.01314109\n -0.9225175   0.5825291  -0.01304154  0.37491497 -0.37053376 -0.20888218\n -1.0596713   1.5670098   0.57272583  1.3895929   0.55373794 -0.2274162\n -1.5938821  -1.081989    0.6925696  -1.307452   -0.745071   -0.31454763\n -0.18321386  1.8907186   0.4405438   1.5854336  -1.405698   -0.40413663\n -0.45831028 -1.3334285  -1.1032896  -0.65287316 -0.23668368 -1.529644\n  1.1334416   0.9414207   0.9909252   0.85138464 -0.32880306  1.0097126\n -1.6619712  -0.68290323  0.902218    0.84549284  0.84821767 -0.5903684\n  0.369186   -0.43195915 -0.23427604  0.66739225 -1.231144    0.1372639\n -0.02913455 -0.6316493   0.94689274  1.3507402  -0.50835323 -0.6915421\n  0.0104849   0.2067438  -0.83454144  1.619512   -0.31260186 -0.05403026\n -0.3931501  -0.55756867  0.6500998  -0.5035867   0.74274546 -0.8373286\n  0.5914503  -0.08630756 -0.8550484  -1.2627065 ]\n(100,)\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n  \"\"\"\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n  # Remove the CWD from sys.path while we load stuff.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Using a pretrained Word Embedding model\n\nIn the above we learnt who to use Gensim Library to train a language model from text.\n\nIn this section we focus on using the language models which are already built and trained with huge amount of data such as the whole corpus of Wikipedia.\n\nDownload a word2vec model in english on Wikipedia from [this link](http://vectors.nlpl.eu/repository/20/3.zip) (596 MB file)","metadata":{"tags":[],"cell_id":"00059-42932642-b183-47b3-858a-a650760c29b8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":" <p style=\"color:red\"> IMPORTANT NOTE: do not run the following cell if you haven't downloaded a word embedding model</p>\n\n\n If you didn't download a model you can continue with the current small model.\n","metadata":{"tags":[],"cell_id":"00060-1ba89a2b-c684-4943-970c-decd9852c440","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The following cell code loads an already trained word2vec model using gensim library. (a pretrained model)","metadata":{"tags":[],"cell_id":"00061-bc944bbf-ee77-4069-b6b3-c2954536589b","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00062-a00bd2c8-f01e-49f9-83ef-6a7bb2c08975","deepnote_to_be_reexecuted":false,"source_hash":"7b2695b","execution_millis":2689,"execution_start":1610716292762,"deepnote_cell_type":"code"},"source":"# Load vectors directly from the file\n\n#Put the address of your downloded language model here \nfrom gensim.models import KeyedVectors\n# Load vectors directly from the file\naddress_of_your_model =\"model.bin\"\nmodel = KeyedVectors.load_word2vec_format(address_of_your_model, binary=True)","execution_count":7,"outputs":[{"name":"stderr","text":"2021-01-15 13:11:32,760 : INFO : loading projection weights from model.bin\n2021-01-15 13:11:35,430 : INFO : loaded (261794, 300) matrix from model.bin\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The word2vec class in gensim library has a function for identifuing the most similar or dissimlar words to a word in it's vocabulary.\n\nTry it by running th e following code cell:","metadata":{"tags":[],"cell_id":"00063-f6f9d0a5-b982-4862-99d8-ce2a915fd2a1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":" <p style=\"color:red\">If you didn't download and load the pretrained model , you can still run the following codes. However it's very probable that your model doesn't work well or doesn't know some words, since it has been trained on a very small corpus</p>","metadata":{"tags":[],"cell_id":"00064-995cd490-b21a-43a5-a9ca-ce0f8eecee58","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"# Load vectors directly from the file\n!wget https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\n!gunzip GoogleNews-vectors-negative300.bin\n#Put the address of your downloded language model here\nfrom gensim.models import KeyedVectors\n# Load vectors directly from the file\naddress_of_your_model =\"GoogleNews-vectors-negative300.bin\"\nmodel = KeyedVectors.load_word2vec_format(address_of_your_model, binary=True)","metadata":{"tags":[],"cell_id":"00066-1e11fbaa-a239-4289-87f6-bff904bd6076","deepnote_to_be_reexecuted":false,"source_hash":"a02d5af9","execution_start":1610716580156,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"--2021-01-15 13:16:20--  https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\nResolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.103.46\nConnecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.103.46|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1647046227 (1.5G) [application/x-gzip]\nSaving to: ‘GoogleNews-vectors-negative300.bin.gz.4’\n\news-vectors-negativ  19%[==>                 ] 298.44M  74.8MB/s    eta 19s    ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00065-0653a1d2-4a2a-44ae-ab90-26eaaa60123e","deepnote_to_be_reexecuted":false,"source_hash":"36b589fa","execution_millis":474,"execution_start":1610716295455,"deepnote_cell_type":"code"},"source":"model.similar_by_word('music')\n","execution_count":8,"outputs":[{"name":"stderr","text":"2021-01-15 13:11:35,453 : INFO : precomputing L2-norms of word weight vectors\n","output_type":"stream"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"[('musician', 0.7037041187286377),\n ('hip-hop', 0.7010228633880615),\n ('soundtrack', 0.7001615762710571),\n ('classical', 0.680194079875946),\n ('orchestral', 0.6711907386779785),\n ('melody', 0.6686183214187622),\n ('song', 0.6630804538726807),\n ('lyric', 0.6580933928489685),\n ('choral', 0.657744824886322),\n ('Music', 0.6570932865142822)]"},"metadata":{}}]},{"cell_type":"markdown","source":"**CODE IT** Using the function `similarity` from gensim library. print the similarity measures of two sets of words according to your model. \n\nCat and Dog \n\nCat and King","metadata":{"tags":[],"cell_id":"00066-990a3ebd-132b-4291-a0cd-044c09469e53","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00067-4e3bdf0f-bc20-48df-b637-ccb9acedee50","deepnote_to_be_reexecuted":false,"source_hash":"a940b6fb","execution_millis":60,"execution_start":1610716295933,"deepnote_cell_type":"code"},"source":"x = 'Cat'\ny = 'Dog'\nz = 'King'\nprint(model.similarity(x,y))\n","execution_count":9,"outputs":[{"name":"stdout","text":"0.52323097\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Analogies:\n\nGensim library provides functionalities for getting analogies from word2vec models.\n\n\nThe `king-man+woman = queen` is a very typical example of how vord embeddings capture semantic dimentions.\n\nImagine a dimention in a 300 dimentional embedding is storing the concept of Royalty in the word `King`. And one other dimention is storing `gender`.\n\nWhat would happen if we substract the vector of `Man` from `King` (getting a vector which keeps the `Royalty` but subtracts `masculinity` from gender) and then add `Women` to the result. We excpect to get Queen (Royality+ feminine) which actualy happens in word embeddings trained are huge amount of text.\n\n\nIn the following we see how we can use the analogy funtionality in gensim library.","metadata":{"tags":[],"cell_id":"00068-c74618b3-2c8b-48a8-8997-d2c771cb1526","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n<img src=\"https://cdn-images-1.medium.com/max/600/1*LdviucnshWgIIcQvhTTF-g.png\" >","metadata":{"tags":[],"cell_id":"00069-d585592f-e3a7-44e4-abe9-ae9a0c4952db","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The following code performs the above vector calculations. King-Man +Woman = Queen","metadata":{"tags":[],"cell_id":"00070-0277a8ba-0bd6-4d45-ad6b-08c701571b99","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00071-ff82cdc3-4de2-4092-8f35-a442a06d5768","deepnote_to_be_reexecuted":false,"source_hash":"756abe0a","execution_millis":57,"execution_start":1610716295995,"deepnote_cell_type":"code"},"source":"model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=3)","execution_count":10,"outputs":[{"name":"stderr","text":"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \"\"\"Entry point for launching an IPython kernel.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"[('monarch', 0.6530377864837646),\n ('queen', 0.5418059825897217),\n ('princess', 0.5147514343261719)]"},"metadata":{}}]},{"cell_type":"markdown","source":"**CODEIT** Using the above example, write a code line which can give the Capital of Belgium as output by knowing the Capital of France. \n\nor in other words:   **France** to **Paris** is **Belgium** to ...\n\n**NOTE:** if you could not load the language model file **write the code as you think it's correct** and get the answer using this demo :https://rare-technologies.com/word2vec-tutorial/#app\n\n","metadata":{"tags":[],"cell_id":"00072-3f95311f-2cf8-4ea1-b618-4987084889f8","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00073-5d7c6fd7-eef8-4fcd-97a2-494612bf867b","deepnote_to_be_reexecuted":false,"source_hash":"1e919cbe","execution_millis":66,"execution_start":1610716296055,"deepnote_cell_type":"code"},"source":"#insert you code here\n\nmodel.wv.most_similar(positive=[\"Paris\", \"Belgium\"], negative=[\"France\"], topn=1)","execution_count":11,"outputs":[{"name":"stderr","text":"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  This is separate from the ipykernel package so we can avoid doing imports until\n","output_type":"stream"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"[('Brussels', 0.6305320858955383)]"},"metadata":{}}]},{"cell_type":"markdown","source":"**CODEIT**    Using the same code try: **Man** is to **Actor** as **Woman** is to ...\n\n\n","metadata":{"tags":[],"cell_id":"00074-b6177933-10bc-45b2-b039-af51f1fcc46a","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00075-38a0fe00-69b4-4f4e-8d7d-3bafed16c8fb","deepnote_to_be_reexecuted":false,"source_hash":"6d3f81be","execution_start":1610716296125,"execution_millis":151,"deepnote_cell_type":"code"},"source":"#insert your code here\nmodel.wv.most_similar(positive=[\"actor\", \"woman\"], negative=[\"man\"], topn=10)","execution_count":12,"outputs":[{"name":"stderr","text":"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \n","output_type":"stream"},{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"[('actress', 0.7306582927703857),\n ('oscar-winner', 0.5388064980506897),\n ('co-star', 0.531090497970581),\n ('Sorvino', 0.5285298824310303),\n ('comedienne', 0.5283129215240479),\n ('Sarandon', 0.5210732221603394),\n ('Blanchett', 0.5175133943557739),\n ('screenwriter', 0.5155196189880371),\n ('Streep', 0.5138664841651917),\n ('Actress', 0.5117000341415405)]"},"metadata":{}}]},{"cell_type":"markdown","source":"**CODEIT**    Using the same code try: **go** is to **going** as **come** is to ...\n\n\n","metadata":{"tags":[],"cell_id":"00076-fd40b2c9-8af7-492b-b94b-b745fc0ecc27","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00077-ca1553fe-7f7d-4f14-af8c-1bed76a9c0b9","deepnote_to_be_reexecuted":false,"source_hash":"e9d95ceb","execution_start":1610716296278,"execution_millis":86,"deepnote_cell_type":"code"},"source":"model.wv.most_similar(positive=[\"going\", \"come\"], negative=[\"go\"], topn=10)","execution_count":13,"outputs":[{"name":"stderr","text":"/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \"\"\"Entry point for launching an IPython kernel.\n","output_type":"stream"},{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"[('COMING', 0.35924386978149414),\n ('ENJOYING', 0.35013359785079956),\n ('SPOTS', 0.34187790751457214),\n ('BROUGHT', 0.33890360593795776),\n ('SURPRISES', 0.3362162709236145),\n ('bc-onbusiness-column-bo', 0.3350003957748413),\n ('UPON', 0.3324851095676422),\n ('ANTICIPATION', 0.3311701714992523),\n ('flurry', 0.3310207724571228),\n ('bring', 0.32899945974349976)]"},"metadata":{}}]},{"cell_type":"markdown","source":"**OBSERVE AND REFLECT : ** if you succeeded in running the above codes you can see that word2vec model have embedded in it some knowledge about the langauge(tenses), knowledge about the world (capital of countries) by observing the conexts of the words in huge amounts of text.  \n\nWhy do you think a languge model might act like the following?\n\n` model.wv.most_similar(positive=[\"doctor\", \"woman\"], negative=[\"man\"], topn=1) = 'nurse' `\n\nRead about [Bias In Language Models](https://towardsdatascience.com/bias-in-natural-language-processing-nlp-a-dangerous-but-fixable-problem-7d01a12cf0f7)\n\n\n\n\n","metadata":{"tags":[],"cell_id":"00078-432391bb-2787-4a4f-ae24-ab31567a25ee","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We are planning a separate session on ethics of NLP - stay tuned!","metadata":{"tags":[],"cell_id":"00079-ea5890bf-108a-43c6-83c3-d123ad9d406a","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The following function in Gensim library finds a word in the list of words which is the most dissimilar to the others:","metadata":{"tags":[],"cell_id":"00080-01970802-79d9-44e0-8fa0-96ae688fec74","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"output_cleared":false,"cell_id":"00081-636d1e8c-8dab-4476-84a9-b898f75ecdb0","deepnote_to_be_reexecuted":false,"source_hash":"5729ba8f","execution_start":1610716296365,"execution_millis":43,"deepnote_cell_type":"code"},"source":"print(model.wv.doesnt_match([\"France\",\"Germany\",\"Britain\",\"cheese\"]))\n\nprint(model.wv.doesnt_match([\"year\",\"book\",\"month\",\"day\"]))","execution_count":14,"outputs":[{"name":"stdout","text":"cheese\nbook\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  \"\"\"Entry point for launching an IPython kernel.\n/opt/venv/lib/python3.7/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n/shared-libs/python3.7/py-core/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n  This is separate from the ipykernel package so we can avoid doing imports until\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Visualization\n\n\nIn order to be able to visualize word embeddings in vector space, we need to use a dimentionality reduction method.\n\nEmbedding projector visualizes the word2vec and any other uploaded word embedding model.\nhttps://projector.tensorflow.org\n\n**Exercise** Load a word2vec model look for a word you find interesting and find the 10 words most close to it isolate them and upload an screen-shot in the next cell. The following cell contains an example of the word `watergate` and the top 10 closest words.\n(You should upload your image) ","metadata":{"tags":[],"cell_id":"00082-a320400d-7dfc-4a31-9e15-459b7300c6fa","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<img src = \"embedding_watergate.png\">","metadata":{"tags":[],"cell_id":"00083-669e47be-4c2c-4569-b96c-7ce85556759f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00084-c4bdaf01-3b56-4f65-9b1f-485fe33ac93b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**IF YOU FANCY** Download one of the gensim models from this repository in your preferred language and run the functions from gensim Word2Vec model class on samples.\nhttp://vectors.nlpl.eu/repository/","metadata":{"tags":[],"cell_id":"00085-f86cd173-70a0-40ae-aac4-4146eb66ea67","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"9f9dc0ff-66a4-4ba9-9fa2-60906bad0cd6","deepnote_execution_queue":[{"cellId":"00066-1e11fbaa-a239-4289-87f6-bff904bd6076","sessionId":"d02c71a7-eb6c-4ab3-afc4-327e36b1c242","msgId":"9e8804f9-9529-4275-a730-038d26b6e7b0"}]}}