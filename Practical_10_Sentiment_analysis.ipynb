{"cells":[{"cell_type":"markdown","source":"# Sentiment analysis","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00000-44bc9024-d3c4-4085-a229-841e61a637f4","output_cleared":false}},{"cell_type":"markdown","source":"** Sentiment analysis** (also known as **opinion mining** or **emotion AI**) -  using natural language processing and computational linguistics to systematically identify, extract, quantify, and study affective states and subjective information. \n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00001-528e434a-a113-4fb2-98dc-5c161f9a02c7","output_cleared":false}},{"cell_type":"markdown","source":"Affective states?\nSubjective information?\nBasically it is about determining whether the writer's attitude towards a particular topic, product, etc. is **positive**, **negative**, or **neutral**.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00002-98fa7b7d-9721-454c-a715-9604c88aee92","output_cleared":false}},{"cell_type":"markdown","source":"**REFLECT AND WRITE (1)**:  What are the applications of sentiment analysis? Think about modern world and how opinion mining can be used (3-5 sentences). ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00003-3920059b-9324-4e7b-87cf-ccc1f609cb53","output_cleared":false}},{"cell_type":"markdown","source":"**YOUR ANSWER**:","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-78a8549a-2719-4940-9ef2-a2a7b1a06898","output_cleared":false}},{"cell_type":"markdown","source":"There are different ways to do sentiment analysis. In this tutorial we will look at some:\n- Vader (lexicon-based)\n- Machine learning (Random Forest algorithm)\n- Neural Networks (RNN/LSTM)\n\nIt is meant to be an introduction so if you want to learn more about some of these topics you may need to consult some other external resources.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-b16594a2-f5f7-46c2-bea9-e5ca62bc0f2c","output_cleared":false}},{"cell_type":"markdown","source":"**IF YOU FANCY** (some more advanced stuff): If you want to continue learning about some cutting edge technique check out this tutorial (there is a Colab notebook too) https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00005-6774e96c-1e95-4a2d-b634-148e4a4a41d0","output_cleared":false}},{"cell_type":"markdown","source":"## METHOD 1: (Darth) VADER","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00005-09c8d356-6df8-44ac-8847-43c92a5fd23e","output_cleared":false}},{"cell_type":"markdown","source":"**VADER (Valence Aware Dictionary and sEntiment Reasoner)** is a lexicon and rule-based sentiment analysis tool specifically attuned to sentiments expressed in social media. \n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00006-bf74ad95-0b28-41d5-999e-c44f8df5eb34","output_cleared":false}},{"cell_type":"markdown","source":"More info: https://github.com/cjhutto/vaderSentiment","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-c4991497-599a-4910-ac82-554be5b4b5f7","output_cleared":false}},{"cell_type":"markdown","source":"**Lexical approach**: it uses words scored as positie and negative, with the scores based on a pre-trained model labeled as such by human reviewers.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00007-3b540562-df42-4923-8b61-b312ef0c4e33","output_cleared":false}},{"cell_type":"markdown","source":"From the authors' GitHub page:\n\"Empirically validated by **multiple independent human judges**, VADER incorporates a \"gold-standard\" sentiment lexicon that is especially attuned to microblog-like contexts\"\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00008-939f8e03-c7f8-41d3-8e49-e5205d928e1f","output_cleared":false}},{"cell_type":"markdown","source":"The VADER sentiment lexicon is sensitive both the **polarity** (positive/negative) and the **intensity** (how positive/how negative) of sentiments expressed in social media contexts, and is also generally applicable to sentiment analysis in other domains.\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00009-7fcd5b66-d359-4145-867a-436adb74123e","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00010-3fbcb853-7280-4d9f-a531-f9d76dc4158f","output_cleared":false,"source_hash":"1a5536b2","execution_millis":3026,"execution_start":1605623339604},"source":"!pip install vaderSentiment\n","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 125 kB 6.6 MB/s \n\u001b[?25hRequirement already satisfied: requests in /opt/venv/lib/python3.7/site-packages (from vaderSentiment) (2.25.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/venv/lib/python3.7/site-packages (from requests->vaderSentiment) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/venv/lib/python3.7/site-packages (from requests->vaderSentiment) (1.26.2)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/venv/lib/python3.7/site-packages (from requests->vaderSentiment) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/venv/lib/python3.7/site-packages (from requests->vaderSentiment) (2020.11.8)\nInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's call the sentiment intensity analyser object:","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00011-cbef9c87-0ba2-4657-81db-d2452a600fd1","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-bd78641c-9692-4733-9738-f1885ef54116","output_cleared":false,"source_hash":"55a14060","execution_millis":28,"execution_start":1605623342665},"source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00013-65982835-0b16-4f20-b080-3a4d3fdb2d0f","output_cleared":false,"source_hash":"cffe7330","execution_millis":2,"execution_start":1605623342705},"source":"def sentiment_analyzer_scores(sentence):\n    score = analyser.polarity_scores(sentence)\n    print(\"{:-<40} {}\".format(sentence, str(score)))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00014-23ca0505-acc7-4bfb-bd7a-8c2238c03818","output_cleared":false,"source_hash":"35a19943","execution_millis":6,"execution_start":1605625577344},"source":"sentiment_analyzer_scores(\"Very nice :(\")","execution_count":null,"outputs":[{"name":"stdout","text":"Very nice :(---------------------------- {'neg': 0.437, 'neu': 0.138, 'pos': 0.425, 'compound': -0.022}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's analyse the result:\n**Positive, Negative and Neutral scores**: the proportion of text that falls in the corresponding categories. \n(our sentence is 67% Positive, 33% Neutral and 0% Negative  - these metrics should add up to 1). ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00015-8980256e-12ba-419b-b42c-fd610358d9b0","output_cleared":false}},{"cell_type":"markdown","source":"In addition to the words themselves carrying some positive or negative emotions, there are certain aspects of data that imply the change in the emotion magnitude. \n\nSome of them are: \n\n- exclamation marks\n- capitalisations (especialle ALL CAPS)\n- degree modifiers (\"very\", \"marginally\")\n- emoji\n- conjunction \"but\"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00018-93e0a225-daca-4a51-be52-4f3d3ff699cb","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00016-3890770d-963e-4e03-b90e-548f6664401a","output_cleared":false,"source_hash":"445fd02e","execution_millis":10,"execution_start":1605623342731},"source":"sentences = [\"VADER is smart, handsome, and funny.\",  #  positive sentence \n             \"VADER is smart, handsome, and funny!\",  # punctuation emphasis\n             \"VADER is very smart, handsome, and funny.\", # degree modifiers\n             \"VADER is VERY SMART, handsome, and FUNNY.\",  # ALLCAPS \n             \"VADER is VERY SMART, handsome, and FUNNY!!!\", # combination of signals\n             \"VADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!\", # booster words & punctuation make this close to ceiling for score\n             \"VADER is not smart, handsome, nor funny.\",  # negation sentence \n             \"The book was good.\",  # positive sentence\n             \"At least it isn't a horrible book.\",  # negated negative sentence with contraction\n             \"The book was only kind of good.\", # qualified positive sentence \n             \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n             \"Today SUX!\",  # negative slang with capitalization emphasis\n             \"Today only kinda sux! But I'll get by, lol\", # mixed sentiment example with slang and constrastive conjunction \"but\"\n             \"Make sure you :) or :D today!\",  # emoticons \n             \"Catch utf-8 emoji such as such as ðŸ’˜ and ðŸ’‹ and ðŸ˜\",  # emojis \n             \"Not bad at all\"  # Capitalized negation\n             ]","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Check how VADER handles all these sentences and adjusts sentiment intensity accordingly.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00020-e9f6b460-9a95-47fa-abe5-71a3d53d20a3","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00017-95b5fe7b-4d76-486f-a0ab-8e0a1d6c9ce8","output_cleared":false,"source_hash":"bee43213","execution_millis":33,"execution_start":1605623342748},"source":"for i in range(len(sentences)):\n    vs = analyser.polarity_scores(sentences[i])\n    print(\"{:-<65} {}\".format(sentences[i],  str(vs)))\n","execution_count":null,"outputs":[{"name":"stdout","text":"VADER is smart, handsome, and funny.----------------------------- {'neg': 0.0, 'neu': 0.254, 'pos': 0.746, 'compound': 0.8316}\nVADER is smart, handsome, and funny!----------------------------- {'neg': 0.0, 'neu': 0.248, 'pos': 0.752, 'compound': 0.8439}\nVADER is very smart, handsome, and funny.------------------------ {'neg': 0.0, 'neu': 0.299, 'pos': 0.701, 'compound': 0.8545}\nVADER is VERY SMART, handsome, and FUNNY.------------------------ {'neg': 0.0, 'neu': 0.246, 'pos': 0.754, 'compound': 0.9227}\nVADER is VERY SMART, handsome, and FUNNY!!!---------------------- {'neg': 0.0, 'neu': 0.233, 'pos': 0.767, 'compound': 0.9342}\nVADER is VERY SMART, uber handsome, and FRIGGIN FUNNY!!!--------- {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.9469}\nVADER is not smart, handsome, nor funny.------------------------- {'neg': 0.646, 'neu': 0.354, 'pos': 0.0, 'compound': -0.7424}\nThe book was good.----------------------------------------------- {'neg': 0.0, 'neu': 0.508, 'pos': 0.492, 'compound': 0.4404}\nAt least it isn't a horrible book.------------------------------- {'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.431}\nThe book was only kind of good.---------------------------------- {'neg': 0.0, 'neu': 0.697, 'pos': 0.303, 'compound': 0.3832}\nThe plot was good, but the characters are uncompelling and the dialog is not great. {'neg': 0.327, 'neu': 0.579, 'pos': 0.094, 'compound': -0.7042}\nToday SUX!------------------------------------------------------- {'neg': 0.779, 'neu': 0.221, 'pos': 0.0, 'compound': -0.5461}\nToday only kinda sux! But I'll get by, lol----------------------- {'neg': 0.127, 'neu': 0.556, 'pos': 0.317, 'compound': 0.5249}\nMake sure you :) or :D today!------------------------------------ {'neg': 0.0, 'neu': 0.294, 'pos': 0.706, 'compound': 0.8633}\nCatch utf-8 emoji such as such as ðŸ’˜ and ðŸ’‹ and ðŸ˜------------------ {'neg': 0.0, 'neu': 0.615, 'pos': 0.385, 'compound': 0.875}\nNot bad at all--------------------------------------------------- {'neg': 0.0, 'neu': 0.513, 'pos': 0.487, 'compound': 0.431}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\"The **compound score** is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate\".\n\nIt is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n\n- positive sentiment: compound score >= 0.05\n- neutral sentiment: (compound score > -0.05) and (compound score < 0.05)\n- negative sentiment: compound score <= -0.05\n\nThe pos, neu, and neg scores are ratios for proportions of text that fall in each category (so these should all add up to be 1... or close to it with float operation). These are the most useful metrics if you want multidimensional measures of sentiment for a given sentence\"\n\nhttps://github.com/cjhutto/vaderSentiment#code-examples","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00020-3e996d42-0598-4e5f-b84a-6ccaea12a47a","output_cleared":false}},{"cell_type":"markdown","source":"**CODE IT (1)**: Write code to score your own 6 sentences with VADER. Try different types of sentences as in the example above (e.g. with degree modifiers, capitalisation, emoji etc.).\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00019-12276e8a-ffd1-45d1-872e-ff810b0f3aa6","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00025-90d81ffd-32e8-4354-8b98-daa7865e6f79","output_cleared":false,"source_hash":"6317bc4","execution_start":1605623342787,"execution_millis":14},"source":"## YOUR CODE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Method 2: Machine learning (sklearn) & Random Forest Classifier","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00020-a3747306-2dc9-416a-aebf-ea4d5e0bc50b","output_cleared":false}},{"cell_type":"markdown","source":"Anohter way to do sentiment analysis is using **machine learning** algorithms.  \nThis notebook is based on the following tutotial: https://stackabuse.com/python-for-nlp-sentiment-analysis-with-scikit-learn/","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00004-be05c5c3-1773-4dc8-85c5-65d00c317af8","output_cleared":false}},{"cell_type":"markdown","source":"- **Data**:  tweets about six US airlines, \n- **Task**: predict whether a tweet contains positive, negative, or neutral sentiment about the airline. \n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00024-c8155e7e-67f1-4099-9d40-3adfa0e4abfb","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00005-2dd0a0c7-ab08-49c1-9b6c-a7f1d37dc2e6","output_cleared":false,"source_hash":"96307baf","execution_millis":2133,"execution_start":1605623342806},"source":"!pip install nltk","execution_count":null,"outputs":[{"name":"stdout","text":"Requirement already satisfied: nltk in /opt/venv/lib/python3.7/site-packages (3.5)\nRequirement already satisfied: click in /opt/venv/lib/python3.7/site-packages (from nltk) (7.1.2)\nRequirement already satisfied: regex in /opt/venv/lib/python3.7/site-packages (from nltk) (2020.11.13)\nRequirement already satisfied: tqdm in /opt/venv/lib/python3.7/site-packages (from nltk) (4.52.0)\nRequirement already satisfied: joblib in /opt/venv/lib/python3.7/site-packages (from nltk) (0.17.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's import packages we are going to need to explore and clean our data before feeding it to a machine learning algorithm.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00025-789cc6c9-a1f3-4e2b-a653-11abc6612fc7","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00000-62ed62d2-385d-471a-bf7d-cef04fb95c03","output_cleared":false,"source_hash":"75ac18c8","execution_millis":2371,"execution_start":1605623344953},"source":"import nltk \nimport pandas as pd \nimport re\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's load the data from an online source:","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00027-11550802-922f-48ef-bb53-d858f976a496","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00006-c93cddf0-fb10-4370-b72d-0b959c804a3a","output_cleared":false,"source_hash":"26743fce","execution_millis":1732,"execution_start":1605623347331},"source":"data_source_url = \"https://raw.githubusercontent.com/kolaveridi/kaggle-Twitter-US-Airline-Sentiment-/master/Tweets.csv\"\nairline_tweets = pd.read_csv(data_source_url)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pandas allows us to look at the first 5 rows to get an idea of what the data looks like. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00030-b338b96c-18a0-4b5c-9490-d003ee7f424f","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00008-83d508e3-2b49-4bde-aee6-83b099b46d13","output_cleared":false,"source_hash":"f2af428d","execution_millis":22,"execution_start":1605623349127},"source":"airline_tweets.head()\n","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":15,"columns":[{"name":"tweet_id","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":570300817074462700,"max":570306133677760500,"histogram":[{"bin_start":570300817074462700,"bin_end":570301348734792500,"count":4},{"bin_start":570301348734792500,"bin_end":570301880395122300,"count":0},{"bin_start":570301880395122300,"bin_end":570302412055452000,"count":0},{"bin_start":570302412055452000,"bin_end":570302943715781800,"count":0},{"bin_start":570302943715781800,"bin_end":570303475376111600,"count":0},{"bin_start":570303475376111600,"bin_end":570304007036441400,"count":0},{"bin_start":570304007036441400,"bin_end":570304538696771200,"count":0},{"bin_start":570304538696771200,"bin_end":570305070357100900,"count":0},{"bin_start":570305070357100900,"bin_end":570305602017430700,"count":0},{"bin_start":570305602017430700,"bin_end":570306133677760500,"count":1}]}},{"name":"airline_sentiment","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"neutral","count":2},{"name":"negative","count":2},{"name":"positive","count":1}]}},{"name":"airline_sentiment_confidence","dtype":"float64","stats":{"unique_count":3,"nan_count":0,"min":0.3486,"max":1,"histogram":[{"bin_start":0.3486,"bin_end":0.41374,"count":1},{"bin_start":0.41374,"bin_end":0.47888000000000003,"count":0},{"bin_start":0.47888000000000003,"bin_end":0.5440200000000001,"count":0},{"bin_start":0.5440200000000001,"bin_end":0.60916,"count":0},{"bin_start":0.60916,"bin_end":0.6743,"count":0},{"bin_start":0.6743,"bin_end":0.7394400000000001,"count":1},{"bin_start":0.7394400000000001,"bin_end":0.8045800000000001,"count":0},{"bin_start":0.8045800000000001,"bin_end":0.86972,"count":0},{"bin_start":0.86972,"bin_end":0.93486,"count":0},{"bin_start":0.93486,"bin_end":1,"count":3}]}},{"name":"negativereason","dtype":"object","stats":{"unique_count":2,"nan_count":3,"categories":[{"name":"Bad Flight","count":1},{"name":"Can't Tell","count":1},{"name":"Missing","count":3}]}},{"name":"negativereason_confidence","dtype":"float64","stats":{"unique_count":3,"nan_count":2,"min":0,"max":1,"histogram":[{"bin_start":0,"bin_end":0.1,"count":1},{"bin_start":0.1,"bin_end":0.2,"count":0},{"bin_start":0.2,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0},{"bin_start":0.5,"bin_end":0.6000000000000001,"count":0},{"bin_start":0.6000000000000001,"bin_end":0.7000000000000001,"count":0},{"bin_start":0.7000000000000001,"bin_end":0.8,"count":1},{"bin_start":0.8,"bin_end":0.9,"count":0},{"bin_start":0.9,"bin_end":1,"count":1}]}},{"name":"airline","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"Virgin America","count":5}]}},{"name":"airline_sentiment_gold","dtype":"object","stats":{"unique_count":0,"nan_count":5,"categories":[{"name":"Missing","count":5}]}},{"name":"name","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"jnardino","count":3},{"name":"cairdin","count":1},{"name":"yvonnalynn","count":1}]}},{"name":"negativereason_gold","dtype":"object","stats":{"unique_count":0,"nan_count":5,"categories":[{"name":"Missing","count":5}]}},{"name":"retweet_count","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":0,"max":0,"histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"text","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"@VirginAmerica What @dhepburn said.","count":1},{"name":"@VirginAmerica plus you've added commercials to the experience... tacky.","count":1},{"name":"3 others","count":3}]}},{"name":"tweet_coord","dtype":"object","stats":{"unique_count":0,"nan_count":5,"categories":[{"name":"Missing","count":5}]}},{"name":"tweet_created","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"2015-02-24 11:35:52 -0800","count":1},{"name":"2015-02-24 11:15:59 -0800","count":1},{"name":"3 others","count":3}]}},{"name":"tweet_location","dtype":"object","stats":{"unique_count":1,"nan_count":4,"categories":[{"name":"Lets Play","count":1},{"name":"Missing","count":4}]}},{"name":"user_timezone","dtype":"object","stats":{"unique_count":3,"nan_count":0,"categories":[{"name":"Pacific Time (US & Canada)","count":3},{"name":"Eastern Time (US & Canada)","count":1},{"name":"Central Time (US & Canada)","count":1}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"tweet_id":570306133677760500,"airline_sentiment":"neutral","airline_sentiment_confidence":1,"negativereason":"nan","negativereason_confidence":"nan","airline":"Virgin America","airline_sentiment_gold":"nan","name":"cairdin","negativereason_gold":"nan","retweet_count":0,"text":"@VirginAmerica What @dhepburn said.","tweet_coord":"nan","tweet_created":"2015-02-24 11:35:52 -0800","tweet_location":"nan","user_timezone":"Eastern Time (US & Canada)","_deepnote_index_column":0},{"tweet_id":570301130888122400,"airline_sentiment":"positive","airline_sentiment_confidence":0.3486,"negativereason":"nan","negativereason_confidence":0,"airline":"Virgin America","airline_sentiment_gold":"nan","name":"jnardino","negativereason_gold":"nan","retweet_count":0,"text":"@VirginAmerica plus you've added commercials to the experience... tacky.","tweet_coord":"nan","tweet_created":"2015-02-24 11:15:59 -0800","tweet_location":"nan","user_timezone":"Pacific Time (US & Canada)","_deepnote_index_column":1},{"tweet_id":570301083672813600,"airline_sentiment":"neutral","airline_sentiment_confidence":0.6837,"negativereason":"nan","negativereason_confidence":"nan","airline":"Virgin America","airline_sentiment_gold":"nan","name":"yvonnalynn","negativereason_gold":"nan","retweet_count":0,"text":"@VirginAmerica I didn't today... Must mean I need to take another trip!","tweet_coord":"nan","tweet_created":"2015-02-24 11:15:48 -0800","tweet_location":"Lets Play","user_timezone":"Central Time (US & Canada)","_deepnote_index_column":2},{"tweet_id":570301031407624200,"airline_sentiment":"negative","airline_sentiment_confidence":1,"negativereason":"Bad Flight","negativereason_confidence":0.7033,"airline":"Virgin America","airline_sentiment_gold":"nan","name":"jnardino","negativereason_gold":"nan","retweet_count":0,"text":"@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse","tweet_coord":"nan","tweet_created":"2015-02-24 11:15:36 -0800","tweet_location":"nan","user_timezone":"Pacific Time (US & Canada)","_deepnote_index_column":3},{"tweet_id":570300817074462700,"airline_sentiment":"negative","airline_sentiment_confidence":1,"negativereason":"Can't Tell","negativereason_confidence":1,"airline":"Virgin America","airline_sentiment_gold":"nan","name":"jnardino","negativereason_gold":"nan","retweet_count":0,"text":"@VirginAmerica and it's a really big bad thing about it","tweet_coord":"nan","tweet_created":"2015-02-24 11:14:45 -0800","tweet_location":"nan","user_timezone":"Pacific Time (US & Canada)","_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n0  570306133677760513           neutral                        1.0000   \n1  570301130888122368          positive                        0.3486   \n2  570301083672813571           neutral                        0.6837   \n3  570301031407624196          negative                        1.0000   \n4  570300817074462722          negative                        1.0000   \n\n  negativereason  negativereason_confidence         airline  \\\n0            NaN                        NaN  Virgin America   \n1            NaN                     0.0000  Virgin America   \n2            NaN                        NaN  Virgin America   \n3     Bad Flight                     0.7033  Virgin America   \n4     Can't Tell                     1.0000  Virgin America   \n\n  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n0                    NaN     cairdin                 NaN              0   \n1                    NaN    jnardino                 NaN              0   \n2                    NaN  yvonnalynn                 NaN              0   \n3                    NaN    jnardino                 NaN              0   \n4                    NaN    jnardino                 NaN              0   \n\n                                                text tweet_coord  \\\n0                @VirginAmerica What @dhepburn said.         NaN   \n1  @VirginAmerica plus you've added commercials t...         NaN   \n2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n3  @VirginAmerica it's really aggressive to blast...         NaN   \n4  @VirginAmerica and it's a really big bad thing...         NaN   \n\n               tweet_created tweet_location               user_timezone  \n0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_id</th>\n      <th>airline_sentiment</th>\n      <th>airline_sentiment_confidence</th>\n      <th>negativereason</th>\n      <th>negativereason_confidence</th>\n      <th>airline</th>\n      <th>airline_sentiment_gold</th>\n      <th>name</th>\n      <th>negativereason_gold</th>\n      <th>retweet_count</th>\n      <th>text</th>\n      <th>tweet_coord</th>\n      <th>tweet_created</th>\n      <th>tweet_location</th>\n      <th>user_timezone</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>570306133677760513</td>\n      <td>neutral</td>\n      <td>1.0000</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>cairdin</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica What @dhepburn said.</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:35:52 -0800</td>\n      <td>NaN</td>\n      <td>Eastern Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>570301130888122368</td>\n      <td>positive</td>\n      <td>0.3486</td>\n      <td>NaN</td>\n      <td>0.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica plus you've added commercials t...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:59 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>570301083672813571</td>\n      <td>neutral</td>\n      <td>0.6837</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>yvonnalynn</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:48 -0800</td>\n      <td>Lets Play</td>\n      <td>Central Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>570301031407624196</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Bad Flight</td>\n      <td>0.7033</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica it's really aggressive to blast...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:15:36 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>570300817074462722</td>\n      <td>negative</td>\n      <td>1.0000</td>\n      <td>Can't Tell</td>\n      <td>1.0000</td>\n      <td>Virgin America</td>\n      <td>NaN</td>\n      <td>jnardino</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>@VirginAmerica and it's a really big bad thing...</td>\n      <td>NaN</td>\n      <td>2015-02-24 11:14:45 -0800</td>\n      <td>NaN</td>\n      <td>Pacific Time (US &amp; Canada)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Every tweet is identified with the \"tweet_id\" number and assigned a sentiment (\"neutral\", \"positive\", or \"negative\"), as well as the airline it is related to, and the tweet text. There are other columns too (e.g. date and time, sentiment confidence, reason for a negative comment).  ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00032-0245f520-d005-4a1d-9f0c-e955778dbf34","output_cleared":false}},{"cell_type":"markdown","source":"Our task is a typical **supervised classification task**. Given a labelled data set (tweets are labelled with their sentiments), we will need to build a model which is able to predict whether a tweet (from a test set on which the model hasnt't been trained) is positive, negative, or neutral. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00033-f92af794-9965-4d3a-a057-7e4e00df3e72","output_cleared":false}},{"cell_type":"markdown","source":"### Know your data","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00034-d28bf8d4-f286-4255-a831-72a6e4f03956","output_cleared":false}},{"cell_type":"markdown","source":"The first step of any data mining project is to explore the data. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00009-91cb1589-0457-42ab-a0ed-aab0a511f26f","output_cleared":false}},{"cell_type":"markdown","source":"Here we set up some visualisation parameters (make a chart bigger). ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00036-b7b24ebb-ce41-4e8e-8285-efe004ef241e","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00010-5004e551-7ef6-4ed8-95a3-e263676a4507","output_cleared":false,"source_hash":"297da4b4","execution_millis":5,"execution_start":1605623349156},"source":"plot_size = plt.rcParams[\"figure.figsize\"] \nprint(plot_size[0]) \nprint(plot_size[1])\n\nplot_size[0] = 8\nplot_size[1] = 6\nplt.rcParams[\"figure.figsize\"] = plot_size \n","execution_count":null,"outputs":[{"name":"stdout","text":"6.0\n4.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, let's plot the data from the spreadsheet and explore it in details.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00038-cebda723-e8f0-473f-b9b2-7346d99d6587","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00011-06076ea7-0b2f-463c-ab85-64ccb31d7393","output_cleared":false,"source_hash":"fe777134","execution_millis":193,"execution_start":1605623349172},"source":"airline_tweets.airline.value_counts().plot(kind='pie', autopct='%1.0f%%')\n","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"<AxesSubplot:ylabel='airline'>"},"metadata":{}},{"data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00012-4c0c53f2-78aa-4e8c-8e61-572e53b64cdc","output_cleared":false,"source_hash":"9a26a303","execution_millis":116,"execution_start":1605623349421},"source":"airline_tweets.airline_sentiment.value_counts().plot(\n    kind='pie', \n    autopct='%1.0f%%', \n    colors=[\"red\", \"yellow\", \"green\"]\n    )\n","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"<AxesSubplot:ylabel='airline_sentiment'>"},"metadata":{}},{"data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{},"output_type":"display_data"}]},{"cell_type":"markdown","source":"Now we know what airlines are discussed and also that there are way more negative tweets than positive or neutral.\n\nWe can also group tweets by airline and see the sentiment breakdown by airline name.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00041-034b1ade-d758-46cd-9b7b-719eee328c9e","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00013-f1034795-b81a-4959-bd5f-64147410da86","output_cleared":false,"source_hash":"eff196f5","execution_millis":370,"execution_start":1605623349546},"source":"airline_sentiment = airline_tweets.groupby(['airline', 'airline_sentiment']).airline_sentiment.count().unstack()\nairline_sentiment.plot(kind='bar')","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"<AxesSubplot:xlabel='airline'>"},"metadata":{}},{"data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"markdown","source":"Install seaborn visualisation library and explore the confidence level of the sentiment labels.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00043-028009ea-2e7b-4b4a-8f96-2fcdaf524232","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00014-89acf9c3-638a-48e9-b700-817cd86fc976","output_cleared":false,"source_hash":"d27c978c","execution_millis":3561,"execution_start":1605623349923},"source":"!pip install seaborn\nimport seaborn as sns\n\nsns.barplot(x='airline_sentiment', y='airline_sentiment_confidence' , data=airline_tweets)\n","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting seaborn\n  Downloading seaborn-0.11.0-py3-none-any.whl (283 kB)\n\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 283 kB 6.3 MB/s \n\u001b[?25hRequirement already satisfied: matplotlib>=2.2 in /opt/venv/lib/python3.7/site-packages (from seaborn) (3.3.3)\nRequirement already satisfied: pandas>=0.23 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.0.5)\nRequirement already satisfied: numpy>=1.15 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.19.4)\nRequirement already satisfied: scipy>=1.0 in /opt/venv/lib/python3.7/site-packages (from seaborn) (1.5.4)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\nRequirement already satisfied: pillow>=6.2.0 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (8.0.1)\nRequirement already satisfied: cycler>=0.10 in /opt/venv/lib/python3.7/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\nRequirement already satisfied: pytz>=2017.2 in /opt/venv/lib/python3.7/site-packages (from pandas>=0.23->seaborn) (2020.4)\nRequirement already satisfied: six>=1.5 in /opt/venv/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\nInstalling collected packages: seaborn\nSuccessfully installed seaborn-0.11.0\n","output_type":"stream"},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"<AxesSubplot:xlabel='airline_sentiment', ylabel='airline_sentiment_confidence'>"},"metadata":{}},{"data":{"text/plain":"<Figure size 576x432 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"markdown","source":"## Dividing dataset into feature and label sets","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00015-8ebf29bb-b595-48f3-a547-ecfb44adde68","output_cleared":false}},{"cell_type":"markdown","source":"A refresher on ML terminology:\n\n- **Features** - input (e.g. a column in your dataset, some property of your training data)\n*(Examples: number of rooms, height of a person, income))*\n- **Labels** - output (what we are trying to predict)\n*(Examples: house price, weight, education level)*\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00016-7a5f3b14-2896-4c02-b690-1017fc7238f7","output_cleared":false}},{"cell_type":"markdown","source":"After exploring the data through visualising it, we need to think which columns we will use as input features and which column values we will try to predict.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00047-a4cac65c-4696-460d-a306-f4c38bbdb7d3","output_cleared":false}},{"cell_type":"markdown","source":"As our task is to classify tweets into three classes (neg, pos, or neutral), we can decide that the labels (output) will be exactly that - the column with the sentiment values. \n\nAs for the input features, let's think: what will we base our decision on?\nMaybe on the tweet date and time? Does the sentiment depend on it? Or on the airline name? \nIn fact, no. The input will be the tweet **text** (column 'text') and the label we want to predict is 'airline_sentiment'). Have a look at the code below and make sure you understand how we assign values to the features and labels sets of data using Pandas iloc (https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html).","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00017-d01e8c2c-e092-474a-8744-c2fb286cffce","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00016-dfcdbb1b-8fb6-4f70-b616-a0a54c6c3adf","output_cleared":false,"source_hash":"5d6a8eeb","execution_millis":4,"execution_start":1605623353490},"source":"features = airline_tweets.iloc[:, 10].values\nlabels = airline_tweets.iloc[:, 1].values","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing (cleaning) data","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00019-7db801a5-f044-4c7f-91d5-a098d0810e04","output_cleared":false}},{"cell_type":"markdown","source":"...with our beloved regular expressions:","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00020-7740de73-517d-422d-9e9b-2df11d39b75a","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00019-9d2d7a74-f48f-4965-bb7c-c1c184119afc","output_cleared":false,"source_hash":"f92bd2a5","execution_millis":429,"execution_start":1605623353515},"source":"processed_features = []\n\nfor sentence in range(0, len(features)):\n    # Remove all the special characters\n    processed_feature = re.sub(r'\\W', ' ', str(features[sentence]))\n\n    # remove all single characters\n    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature)\n\n    # Remove single characters from the start\n    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) \n\n    # Substituting multiple spaces with single space\n    processed_feature = re.sub(r'\\s+', ' ', processed_feature, flags=re.I)\n\n    # Removing prefixed 'b'\n    processed_feature = re.sub(r'^b\\s+', '', processed_feature)\n\n    # Converting to Lowercase\n    processed_feature = processed_feature.lower()\n\n    processed_features.append(processed_feature)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see what has been done here.\n\n1. Removing all the special characters\n2. Remove all the single characters left as a result of Step 1.\n(e.g. if we remove special character ' from Jack's and replace it with space, we are left with Jack s. Here s has no meaning, so we remove it by replacing all single characters with a space)\n3. If we replace all single characters with space, multiple spaces are created. => let's replace all multiple spaces with single spaces \n4. Lowercase everything","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00053-23ac81d9-5231-47d4-9231-20ea0b7f8528","output_cleared":false}},{"cell_type":"markdown","source":"## Vectorising text","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00022-e0714eb7-92e5-444a-88d4-68556ec8f22c","output_cleared":false}},{"cell_type":"markdown","source":"As we know, machine learning systems only work with numbers, not raw texts. We now have to convert our text data into numerical representation. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00023-74470173-a3c8-41e3-863a-a6f6a2de7550","output_cleared":false}},{"cell_type":"markdown","source":"You are already familiar with the **TF-IDF vectoriser**. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00024-991489eb-4380-4ab5-8efb-973b247c65a7","output_cleared":false}},{"cell_type":"markdown","source":"**REFLECT AND WRITE (2)**: what does TF-IDF vectoriser do? How is it different from other vectorising methods?","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00059-98e41dbd-0991-4ba9-8320-2236b8799aff","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00025-17cefa62-87df-431c-ada3-1f79b4c91452","output_cleared":false,"source_hash":"a7e3ed37","execution_millis":1188,"execution_start":1605623353951},"source":"nltk.download ('stopwords')\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer (max_features=2500, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))\nprocessed_features = vectorizer.fit_transform(processed_features).toarray()","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's look at some parameters:\n- **max_features** (set to 2500): the vectoriser will only use the 2500 most frequently occurring words to create a bag of words feature vector. \n- **max_df** (set to 0.8): only use the words occuring occur in a maximum of 80% of the documents. \n- **min-df** (set to 7): only use words that occur in at least 7 documents\nThis is one of the ways to exclude words that may be too common to be useful for our task.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00027-bc574efc-6afe-4069-92ed-7823a4eb98e4","output_cleared":false}},{"cell_type":"markdown","source":"## Dividing data into training and testing sets","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00028-ba824a91-c36f-4fc1-8678-778602b334d1","output_cleared":false}},{"cell_type":"markdown","source":"- **Training set** - used to train the model\n- **Test set**  - used to test the model's performance","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00029-9e5fe4b4-3205-4107-9b2c-6c0a262e5936","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00030-ba59a83a-2039-4898-9f52-55e16aebfacc","output_cleared":false,"source_hash":"5a232803","execution_millis":574,"execution_start":1605623355153},"source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **test_size** (set to 0.2): our data set will be split into two sets of 80% (for training) and 20% data (for testing).\n- **random_state** parameter is used to ensure that every time you run your code the split will end up in subsets of data containing identical values. It can be set to anything but commonly used value is 0 (which means \"I want this code to be reproducible). Some ML folks also like to use 42 (https://www.independent.co.uk/life-style/history/42-the-answer-to-life-the-universe-and-everything-2205734.html). ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00031-d4f89992-93a9-4f73-82a9-1a369d11778e","output_cleared":false}},{"cell_type":"markdown","source":"## Machine learning in action","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00032-008d4799-157d-4d24-9431-3f6045c5f94b","output_cleared":false}},{"cell_type":"markdown","source":"Now that we have split our data into training and test sets, we will use a machine learning algorithm to learn from it.\nAlthough we can use any ML algorithm, here we will use **Random Forest**.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00033-a977eeaf-c354-4227-b7e9-9832950760d7","output_cleared":false}},{"cell_type":"markdown","source":"![](https://i.pinimg.com/564x/55/cc/a7/55cca7c380856eb2300caf24fd95097c.jpg)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00065-af0c1008-4a01-4d1f-a688-a251e15ed062","output_cleared":false}},{"cell_type":"markdown","source":"(Image source: https://www.pinterest.com.au/pin/458733912018966676/)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00066-1a5e7449-e392-4449-a114-496aab287fc6","output_cleared":false}},{"cell_type":"markdown","source":"Do you know about **decision trees**?\nThis algorithm is a supervised learning one. \nWe create a training model to be used to predict class or value of target variables by learning **decision rules** inferred from the training data.\n\nThe algorithm solves the problem using **tree representation**. Each internal node of the tree corresponds to an attribute, and each leaf node corresponds to a class label.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00067-5fcd7acf-9002-4bd4-ab7a-4b9a2f781ef9","output_cleared":false}},{"cell_type":"markdown","source":"**Random forest** algorithm combines the results of multiple decision trees. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00068-f3cd8753-b9ae-4fd7-9771-709833191b3e","output_cleared":false}},{"cell_type":"markdown","source":"A short video on decision trees and random forests: https://www.youtube.com/watch?v=J4Wdy0Wc_xQ&t=49s","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00069-a236ec12-3697-45d5-b789-764545a296c1","output_cleared":false}},{"cell_type":"markdown","source":"**RandomForestClassifier** class of the **sklearn.ensemble** module can be used to train a model with the random forest algorithm. \nWe need to call the **fit** method on the RandomForestClassifier and pass it our **training features and labels** as parameters. \n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00034-f486ae54-9a38-447a-9e8b-8d631bf92107","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00034-8c875931-1876-4ff2-a0f6-b39af319d30b","output_cleared":false,"source_hash":"51b151a","execution_millis":66436,"execution_start":1605623355732},"source":"from sklearn.ensemble import RandomForestClassifier\n\ntext_classifier = RandomForestClassifier(n_estimators=100, random_state=0)\ntext_classifier.fit(X_train, y_train)\n","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"RandomForestClassifier(random_state=0)"},"metadata":{}}]},{"cell_type":"markdown","source":"- **n_estimators** = number of trees in your forest (Random Forest is an ensemble method comprising of creating multiple decision trees).\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00067-8124bd24-0a04-4801-a022-7fe973624c2f","output_cleared":false}},{"cell_type":"markdown","source":"## Predictions ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00068-fa760912-f6a0-4813-b65e-a209abc17d7e","output_cleared":false}},{"cell_type":"markdown","source":"Once the model has been trained, we can now use the model to make predictions (is this tweet negative? positive? neutral?). \nTo do so, we need to call the **predict** method on the object of the RandomForestClassifier class that we used for training. \n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00036-368fff5e-6528-47b2-ace8-168647fb2dae","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00037-1f4f42c6-3deb-49cd-bda1-9068575407d2","output_cleared":false,"source_hash":"1214093a","execution_millis":453,"execution_start":1605623422211},"source":"predictions = text_classifier.predict(X_test)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Done!\nNow we need to evaluate the performance of our model.\nYou may remember the classification metrics we can use, such as:\n- confusion matrix\n- F1 measure\n- accuracy\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00071-2ecdbee2-dafc-40e3-aac0-948687412f10","output_cleared":false}},{"cell_type":"markdown","source":"**CONFUSION MATRIX, PRECISION AND RECALL REFRESHER**\n![](https://miro.medium.com/max/4420/1*btcfBuM5Eqqc6rJ3iw3sNQ.png)\n(Image source (and a good article on the topic): https://medium.com/@alon.lek/should-i-look-at-precision-recall-or-specificity-sensitivity-3946158aace1)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00077-7787d9d0-9e32-456f-b825-3b2d26d43582","output_cleared":false}},{"cell_type":"markdown","source":"F-score is a measure of the model's accuracy which combines precision and recall. \nRead more: https://deepai.org/machine-learning-glossary-and-terms/f-score\n![]()","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00078-8609ccab-ae02-41ac-bd31-ff2bd28b7eca","output_cleared":false}},{"cell_type":"markdown","source":"![](image-20201117-095812.png)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00079-6a339c83-2580-4626-93da-7f17de276d9f","output_cleared":false}},{"cell_type":"markdown","source":"Let's see how our model performed.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00080-5aac0219-bd30-475f-8e73-a7a29b94763c","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00038-076cb45d-02d6-4cfb-8179-21d5f9cbd0a1","output_cleared":false,"source_hash":"b009a922","execution_millis":69,"execution_start":1605623422715},"source":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint(accuracy_score(y_test, predictions))","execution_count":null,"outputs":[{"name":"stdout","text":"[[1727  106   37]\n [ 323  253   38]\n [ 138   58  248]]\n              precision    recall  f1-score   support\n\n    negative       0.79      0.92      0.85      1870\n     neutral       0.61      0.41      0.49       614\n    positive       0.77      0.56      0.65       444\n\n    accuracy                           0.76      2928\n   macro avg       0.72      0.63      0.66      2928\nweighted avg       0.75      0.76      0.74      2928\n\n0.7609289617486339\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Voila! Our algorithm has achieved an accuracy of 75.30.\nThere may be ways to improve it (add more data, better feature engineering, model parameter tuning etc.). \nBut there are also other ways to do sentiment analysis, so let's move on for now!\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00073-c76a7aeb-6503-4410-adf6-619f44afe580","output_cleared":false}},{"cell_type":"markdown","source":"## Method 3 - Neural Networks ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00039-923a0373-d1f1-430f-872f-9ce4f3771530","output_cleared":false}},{"cell_type":"markdown","source":"Another method we will look into today is using neural networks.\nWe will train a sentiment classifier for movie reviews in IMDB data set, using **Recurrent Neural Networks**.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00075-a9d64784-51ec-4568-b1f1-a858875dfd5e","output_cleared":false}},{"cell_type":"markdown","source":"Video on RNNs: https://www.youtube.com/watch?v=LHXXI4-IEns","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00085-10d3e34c-e1c8-40ab-8a89-6654c82ea94c","output_cleared":false}},{"cell_type":"markdown","source":"The code below is based on this tutorial: https://towardsdatascience.com/a-beginners-guide-on-sentiment-analysis-with-rnn-9e100627c02e","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00045-8a7a1644-3221-4158-8803-25d631d13666","output_cleared":false}},{"cell_type":"markdown","source":"Let's start with installing Keras and Tensorflow. \nTensorflow is an open source library for numerical computation that makes machine learning and neural network training and running faster and easier.\nKeras, in turn, is a deep learning framework built on top of Tensorflow.\n\nKeras provides some toy datasets (https://keras.io/api/datasets/), including a sentiment analysis one - https://keras.io/api/datasets/imdb/.\n\"This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers).\"","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00087-6b02c95d-a2c8-4d96-b791-2a8246fbf779","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00045-cbe4bbf3-09c2-4842-a7a4-e20ac144e536","output_cleared":false,"source_hash":"13247c78","execution_millis":10141,"execution_start":1605623422829},"source":"!pip install keras\n!pip install tensorflow\nimport tensorflow as tf\nfrom keras.datasets import imdb","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting keras\n  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\nCollecting pyyaml\n  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 269 kB 10.4 MB/s \n\u001b[?25hCollecting h5py\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.0 MB 19.7 MB/s \n\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /opt/venv/lib/python3.7/site-packages (from keras) (1.19.4)\nRequirement already satisfied: scipy>=0.14 in /opt/venv/lib/python3.7/site-packages (from keras) (1.5.4)\nCollecting cached-property; python_version < \"3.8\"\n  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\nBuilding wheels for collected packages: pyyaml\n  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44619 sha256=5f32f2348c354df6103cc017d298adc36abdcec392e7277eab0d35ebd022452a\n  Stored in directory: /home/jovyan/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\nSuccessfully built pyyaml\nInstalling collected packages: pyyaml, cached-property, h5py, keras\nSuccessfully installed cached-property-1.5.2 h5py-3.1.0 keras-2.4.3 pyyaml-5.3.1\nCollecting tensorflow\n  Downloading tensorflow-2.3.1-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n\u001b[K  ^C\n\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\n\u001b[?25h","output_type":"stream"},{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-bd220b674f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install keras'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}]},{"cell_type":"markdown","source":"In this dataset, words are indexed by overall frequency in the dataset.\nE.g. integer \"3\" encodes the 3rd most frequent word in the data. \nThis helps us do some quick filtering like \"only consider the top 10,000 most common words, except the top 20 most common words\".\n\nBelow we set up the vocabulary size - the 'num_words\" parameters will do exactly that, command the functiion to only use the top 5000 most frequently used words in the dataset.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00089-d3257186-dbe6-4481-a0c9-5c440ba82490","output_cleared":false}},{"cell_type":"markdown","source":"The load function returns a tuple of Numpy arrays: (x_train, y_train), (x_test, y_test).\n(Read more: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00090-15497602-388d-412c-8ea7-c92e31978a5f","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00046-f5b5819c-3b82-4bc3-b07b-40a75d55d168","output_cleared":false,"source_hash":"59308d79","execution_millis":8240},"source":"vocabulary_size = 5000\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)\nprint('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's have a look at a sample review and its label.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00092-c965b5f1-75b3-4211-b740-87258769b230","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00047-c38dced8-0119-495a-bfd1-c2cdcb6bf8b3","output_cleared":false,"source_hash":"78e02a3c","execution_millis":4},"source":"print('---review---')\nprint(X_train[6])\nprint('---label---')\nprint(y_train[6])","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The review is represented by a sequence of integers. Each of the integers is a word ID. These IDs were assigned to all individual words.\nThe label, in turn, is an integer (0 for negative, 1 for positive).\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00094-f0b2b817-73f9-4c85-b3b1-20ad1669eca5","output_cleared":false}},{"cell_type":"markdown","source":"**imdb.get_word_index()** returns the dictionary where words are mapped to ID integers, and we cal always use it to get a word from its index.\nto map the review back to the original words.\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00095-ca6cb262-e78b-499d-95f7-838cbbb66395","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00048-6acead2e-48f6-4a55-89bf-81ebb433f2f0","output_cleared":false,"source_hash":"241304bf","execution_millis":181},"source":"word2id = imdb.get_word_index()\nid2word = {i: word for word, i in word2id.items()}\nprint('---review with words---')\nprint([id2word.get(i, ' ') for i in X_train[6]])\nprint('---label---')\nprint(y_train[6])","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00049-102f0aa1-6a90-4f68-b477-9ae5bfe418d2","output_cleared":false,"source_hash":"a4e8272b","execution_millis":3},"source":"print('Maximum review length: {}'.format(\nlen(max((X_train + X_test), key=len))))","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00050-4278af8b-0e4c-4254-9749-cdc987f90416","output_cleared":false,"source_hash":"bea4d806","execution_millis":4},"source":"print('Minimum review length: {}'.format(\nlen(min((X_test + X_test), key=len))))\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To feed our data into RNN, all input documents must have the same length. \nLonger reviews will be trunkated, shorter reviws - padded with zeros. \n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00099-d889d73d-b4de-4c73-898a-3d30f07878fd","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00051-a4addb7e-c66d-44fe-afd5-75b55f8353e7","output_cleared":false,"source_hash":"6d644bb6","execution_millis":1545},"source":"from keras.preprocessing import sequence\nmax_words = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's design our RNN model for sentiment analysis!\nFirst, we import some layers from Keras.\nReminder: our **input** is a sequence of words (technically, integer word IDs) of maximum length = max_words, and our **output** is a binary sentiment label (0 or 1).\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00101-25bf6f0e-7992-44cb-953e-5a9a11d3c31b","output_cleared":false}},{"cell_type":"markdown","source":"Keras offers an **Embedding layer** that can be used for neural networks on text data.\nThe input data for this layer must be represented as integers. We have done it already. \nThe Embedding layer is initialized with random weights and will learn an embedding for each of the words in the training dataset.\n\n\n\n\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00102-d0e55a26-60f8-4bc5-ab78-a71be5c9e8f6","output_cleared":false}},{"cell_type":"markdown","source":"First layer - **Embedding layer** - must have 3 arguments set:\n\n- **input_dim**: size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n- **output_dim**: size of the vector space in which words will be embedded (choose any, try different values)\n- **input_length**: length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00103-d552ecd4-be29-49e7-9e83-6d02955225fc","output_cleared":false}},{"cell_type":"markdown","source":"Second layer we add is **LSTM - Long Short-Term Memory**.\n\n\"Recurrent neural networks are used for \"things\" that happen recurrently so one thing after the other (e.g. time series, but also words). Long Short-Term Memory networks (LSTM) are a specific type of Recurrent Neural Network (RNN) that are capable of learning the relationships between elements in an input sequence. In our case the elements are words. So our next layer is an LSTM layer with 100 memory units\".\n(https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks)\n\n\nMore about LSTM: http://colah.github.io/posts/2015-08-Understanding-LSTMs/\nAnd an illustrated guide here: https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21\n\n\nThe output of the embedding layer is fed to the LSTM layer. \nFinally, there is a Dense layer with Sigmoid activation function which essentially does the class prediction. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00104-e8c1cc47-070d-4862-ac02-7b34b8df2e0a","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00052-e56254db-55f8-4575-99d2-c2ddcf54c7ad","output_cleared":false,"source_hash":"dec37edd","execution_millis":327},"source":"from keras import Sequential #Sequential model is a sequence of layers\nfrom keras.layers import Embedding, LSTM, Dense, Dropout\nembedding_size=32\nmodel=Sequential()\n# we have initialised a Sequential model and now start adding layers:\nmodel.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\nmodel.add(LSTM(100))\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's now compile our model. We specify the **loss function** and **optimizer** we want to use for training, and any **evaluation metrics** we want to measure. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00106-69f515c2-d027-4fd2-8f15-11d5accf4a29","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00053-45f1abc4-1467-4c2c-835d-0d93dbec3620","output_cleared":false,"source_hash":"c29f3392","execution_millis":4},"source":"model.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy'])","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When the model is compiled, we can start training. \n**Batch size** and **number of training epochs** have to be specified: together with our model architecture these parameteres will determine the training time.\n\nTraining may take a while - be patient and relax :)","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00108-4803653c-33da-42f7-ad22-74f128abc04c","output_cleared":false}},{"cell_type":"markdown","source":"During model training we need to observe the loss function: it should constantly be going down. It means that our model is improving. \nWe show the model our dataset 3 times (**epochs** parameter). \nThe **batch size** (we set it to 64) defines how many reviews (data samples) the model will see at once. ","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00109-a2315cab-2975-466c-9fa8-ef3f94d55215","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00054-50add25d-5c2b-4a6f-aaa0-06b019cfd90a","output_cleared":false,"source_hash":"95769e7d","execution_millis":1459530},"source":"batch_size = 64\nnum_epochs = 3\nX_valid, y_valid = X_train[:batch_size], y_train[:batch_size]\nX_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]\nmodel.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model is trained!\nLet's check how well it performs on unseen test data.","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00110-57d2ae93-52e6-4613-b22d-daaafde709f2","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00055-d8b068e5-2082-46a2-ab4a-6d784c6171bc","output_cleared":false,"source_hash":"c0ecc7ce","execution_millis":160643},"source":"scores = model.evaluate(X_test, y_test, verbose=0)\nprint('Test accuracy:', scores[1])","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are several ways in which we can build the model. We can experiments with different layers and parameters while considering such factors as training time and the possibility of overfitting. \n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00112-0cebba1c-6371-4c2e-8e08-48d32ae2d9a7","output_cleared":false}},{"cell_type":"markdown","source":"**CODE IT (2)**: experiment with a different network architecture and parameter tuning and see if  you can get a more accurate sentiment classification model.\n\nExamples of what you can try (choose any):\n\n- add Flatten layer before the final Dense one with model.add(Flatten())\n- replace LSTM with SimpleRNN with *model.add(layers.SimpleRNN(15))*\n- change number of epochs\n- change number of vector dimensions (embedding size)\n- change batch size\n- change number of LSTM or SimpleRNN units\n\n","metadata":{"deepnote_cell_type":"markdown","tags":[],"cell_id":"00115-6fe3a848-cbf2-42fa-a5d3-f3a54de1e5ea","output_cleared":false}},{"cell_type":"code","metadata":{"deepnote_cell_type":"code","tags":[],"cell_id":"00115-94dfd133-3160-4393-a6c3-35a3c9fbf228","output_cleared":false,"source_hash":"75d640a1"},"source":"#YOUR CODE (there is no right answer)","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"7ee4fa74-cc74-419f-9569-536f7301ace1","deepnote_execution_queue":[]}}